"""
ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ - ì‹¤ì „ ë¶„ì„ ëª¨ë“ˆ
ë°ì´í„° ìƒì„±, ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ëª¨ë“ˆ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
import os
from datetime import datetime, timedelta
import joblib

warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
if platform.system() == 'Darwin':  # macOS
    plt.rc('font', family='AppleGothic')
elif platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:  # Linux
    plt.rc('font', family='NanumGothic')

# ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rc('axes', unicode_minus=False)

# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(os.path.join('output'), exist_ok=True)
os.makedirs(os.path.join('data'), exist_ok=True)
os.makedirs(os.path.join('models'), exist_ok=True)


def load_and_prepare_data(data_dir=None):
    if data_dir is None:
        data_dir = os.path.join('data')
    """ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëª¨ë¸ë§ì„ ìœ„í•´ ì¤€ë¹„"""

    # íŒŒì¼ í™•ì¸
    required_files = ['energy_demand.csv', 'renewable_policy.csv', 'electricity_market.csv']
    files_exist = all(os.path.exists(os.path.join(data_dir, f)) for f in required_files)

    if not files_exist:
        raise FileNotFoundError("í•„ìš”í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. data/ í´ë”ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
    demand_df = pd.read_csv(os.path.join(data_dir, 'energy_demand.csv'), parse_dates=['timestamp'])
    policy_df = pd.read_csv(os.path.join(data_dir, 'renewable_policy.csv'), parse_dates=['timestamp'])
    market_df = pd.read_csv(os.path.join(data_dir, 'electricity_market.csv'), parse_dates=['timestamp'])

    # ë°ì´í„° ë³‘í•©
    merged_df = demand_df.merge(market_df, on='timestamp', how='left')
    merged_df = pd.merge_asof(merged_df.sort_values('timestamp'),
                              policy_df.sort_values('timestamp'),
                              on='timestamp',
                              direction='backward')
    merged_df = merged_df.ffill().fillna(0)

    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(merged_df)} ë ˆì½”ë“œ")

    return merged_df

def create_sequences(data, sequence_length=24, target_col='demand_mw'):
    """ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„±"""

    # timestamp ì—´ì´ ìˆìœ¼ë©´ ì œì™¸
    cols_to_drop = [col for col in ['timestamp', target_col] if col in data.columns]
    features = data.drop(columns=cols_to_drop).values
    targets = data[target_col].values

    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(features[i:i+sequence_length])
        y.append(targets[i+sequence_length])

    return np.array(X), np.array(y)

def build_lstm_model(input_shape):
    """LSTM ëª¨ë¸ êµ¬ì¶•"""
    model = keras.Sequential([
        keras.layers.LSTM(64, return_sequences=True, input_shape=input_shape),
        keras.layers.Dropout(0.2),
        keras.layers.LSTM(32, return_sequences=False),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(16, activation='relu'),
        keras.layers.Dense(1)
    ])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])
    return model

def build_gru_model(input_shape):
    """GRU ëª¨ë¸ êµ¬ì¶•"""
    model = keras.Sequential([
        keras.layers.GRU(64, return_sequences=True, input_shape=input_shape),
        keras.layers.Dropout(0.2),
        keras.layers.GRU(32, return_sequences=False),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(16, activation='relu'),
        keras.layers.Dense(1)
    ])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])
    return model

def build_simple_rnn_model(input_shape):
    """ê°„ë‹¨í•œ RNN ëª¨ë¸ êµ¬ì¶• (ë¹„êµìš©)"""
    model = keras.Sequential([
        keras.layers.SimpleRNN(32, return_sequences=True, input_shape=input_shape),
        keras.layers.Dropout(0.2),
        keras.layers.SimpleRNN(16),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(8, activation='relu'),
        keras.layers.Dense(1)
    ])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                  loss='mse',
                  metrics=['mae'])
    return model

def save_models(models, scaler, save_dir=None):
    if save_dir is None:
        save_dir = os.path.join('models')
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì €ì¥"""
    print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")

    for name, model in models.items():
        model_path = os.path.join(save_dir, f'{name}_model.keras')
        model.save(model_path)
        print(f"âœ… {name} ëª¨ë¸ ì €ì¥: {model_path}")

    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    scaler_path = os.path.join(save_dir, 'scaler.pkl')
    joblib.dump(scaler, scaler_path)
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_path}")

    print(f"\nëª¨ë“  ëª¨ë¸ì´ {save_dir}/ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def load_models(model_dir=None):
    if model_dir is None:
        model_dir = os.path.join('models')
    """ì €ì¥ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œ"""
    print("\nğŸ“‚ ì €ì¥ëœ ëª¨ë¸ ë¡œë”© ì¤‘...")

    models = {}
    model_files = ['LSTM_model.keras', 'GRU_model.keras', 'RNN_model.keras']

    for model_file in model_files:
        model_path = os.path.join(model_dir, model_file)
        if os.path.exists(model_path):
            model_name = model_file.replace('_model.keras', '')
            models[model_name] = keras.models.load_model(model_path)
            print(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ: {model_path}")
        else:
            print(f"âš ï¸ {model_file} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = os.path.join(model_dir, 'scaler.pkl')
    scaler = None
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_path}")
    else:
        print("âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if not models:
        raise FileNotFoundError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•˜ì„¸ìš”.")

    return models, scaler

def train_and_evaluate_models(data, epochs=10, save=True):
    """ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    print("\n" + "="*50)
    print("ëª¨ë¸ í•™ìŠµ ë° í‰ê°€")
    print("="*50)

    # ë°ì´í„° ì „ì²˜ë¦¬
    scaler = MinMaxScaler()
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    data_scaled = data.copy()
    data_scaled[numeric_columns] = scaler.fit_transform(data[numeric_columns])

    # ì‹œí€€ìŠ¤ ìƒì„±
    X, y = create_sequences(data_scaled, sequence_length=24)

    # í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
    n_samples = len(X)
    train_size = int(0.7 * n_samples)
    val_size = int(0.15 * n_samples)

    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size+val_size]
    y_val = y[train_size:train_size+val_size]
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]

    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"ê²€ì¦ ë°ì´í„°: {X_val.shape}")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    models = {}
    histories = {}

    # LSTM ëª¨ë¸ í•™ìŠµ
    print("\nğŸ”„ LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
    lstm_model = build_lstm_model((X_train.shape[1], X_train.shape[2]))
    lstm_history = lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=32,
        verbose=1
    )
    models['LSTM'] = lstm_model
    histories['LSTM'] = lstm_history

    # GRU ëª¨ë¸ í•™ìŠµ
    print("\nğŸ”„ GRU ëª¨ë¸ í•™ìŠµ ì¤‘...")
    gru_model = build_gru_model((X_train.shape[1], X_train.shape[2]))
    gru_history = gru_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=32,
        verbose=1
    )
    models['GRU'] = gru_model
    histories['GRU'] = gru_history

    # ê°„ë‹¨í•œ RNN ëª¨ë¸ í•™ìŠµ (ì˜µì…˜)
    print("\nğŸ”„ Simple RNN ëª¨ë¸ í•™ìŠµ ì¤‘...")
    rnn_model = build_simple_rnn_model((X_train.shape[1], X_train.shape[2]))
    rnn_history = rnn_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=32,
        verbose=1
    )
    models['RNN'] = rnn_model
    histories['RNN'] = rnn_history

    # ëª¨ë¸ í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    print("-" * 50)

    results = {}
    for name, model in models.items():
        test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
        results[name] = {'loss': test_loss, 'mae': test_mae}
        print(f"{name:10s} - Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}")

    # ëª¨ë¸ ì €ì¥
    if save:
        save_models(models, scaler)

    # ì˜ˆì¸¡ ì‹œê°í™”
    visualize_predictions(models, X_test, y_test, scaler)
    visualize_training_history(histories)

    return models, histories, results, scaler

def visualize_predictions(models, X_test, y_test, scaler):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""

    n_models = len(models)
    fig, axes = plt.subplots(n_models, 1, figsize=(15, 5*n_models))

    if n_models == 1:
        axes = [axes]

    # ìƒ˜í”Œ êµ¬ê°„ ì„ íƒ (ì²˜ìŒ 200ê°œ)
    n_show = min(200, len(y_test))

    for idx, (name, model) in enumerate(models.items()):
        # ì˜ˆì¸¡
        pred = model.predict(X_test)

        # ì‹œê°í™”
        ax = axes[idx]
        ax.plot(y_test[:n_show], label='ì‹¤ì œê°’', alpha=0.7, linewidth=1.5)
        ax.plot(pred[:n_show], label=f'{name} ì˜ˆì¸¡', alpha=0.7, linewidth=1.5)
        ax.set_title(f'{name} ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼')
        ax.set_xlabel('ì‹œê°„')
        ax.set_ylabel('ì „ë ¥ ìˆ˜ìš” (ì •ê·œí™”)')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # ìºí”ì…˜ ì¶”ê°€
        mae = np.mean(np.abs(pred[:n_show].flatten() - y_test[:n_show]))
        ax.text(0.5, -0.15, f'{name} ëª¨ë¸ì˜ ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡: 24ì‹œê°„ ì´ì „ ë°ì´í„°ë¡œ ë‹¤ìŒ ì‹œê°„ ì˜ˆì¸¡\nMAE: {mae:.4f} (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•)',
                ha='center', fontsize=9, style='italic', wrap=True, transform=ax.transAxes)

    plt.tight_layout()
    plt.savefig(os.path.join('output', 'model_predictions.png'), dpi=150, bbox_inches='tight')
    plt.show()

    print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” ì™„ë£Œ ({os.path.join('output', 'model_predictions.png')})")

def visualize_training_history(histories):
    """í•™ìŠµ ê³¼ì • ì‹œê°í™”"""

    n_models = len(histories)
    fig, axes = plt.subplots(2, n_models, figsize=(5*n_models, 10))

    if n_models == 1:
        axes = axes.reshape(-1, 1)

    for idx, (name, history) in enumerate(histories.items()):
        # Loss ê·¸ë˜í”„
        ax = axes[0, idx]
        ax.plot(history.history['loss'], label='Train Loss')
        ax.plot(history.history['val_loss'], label='Val Loss')
        ax.set_title(f'{name} - Loss')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.text(0.5, -0.2, 'í•™ìŠµ ì†ì‹¤: í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì˜ ì˜¤ì°¨ ê°ì†Œ ì¶”ì´',
                ha='center', fontsize=9, style='italic', wrap=True, transform=ax.transAxes)

        # MAE ê·¸ë˜í”„
        ax = axes[1, idx]
        ax.plot(history.history['mae'], label='Train MAE')
        ax.plot(history.history['val_mae'], label='Val MAE')
        ax.set_title(f'{name} - MAE')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('MAE')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.text(0.5, -0.2, 'í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ í‰ê· ì ì¸ ì°¨ì´',
                ha='center', fontsize=9, style='italic', wrap=True, transform=ax.transAxes)

    plt.tight_layout()
    plt.savefig(os.path.join('output', 'training_history.png'), dpi=150, bbox_inches='tight')
    plt.show()

    print(f"âœ… í•™ìŠµ ê³¼ì • ì‹œê°í™” ì™„ë£Œ ({os.path.join('output', 'training_history.png')})")

def analyze_policy_impact(data, model):
    """ì •ì±… ì˜í–¥ ë¶„ì„"""
    print("\n" + "="*50)
    print("ì •ì±… ì˜í–¥ ë¶„ì„")
    print("="*50)

    # ì •ì±… ë³€í™” ì‹œì  ì°¾ê¸°
    policy_cols = ['renewable_target', 'subsidy_rate', 'carbon_tax']

    fig, axes = plt.subplots(len(policy_cols), 1, figsize=(15, 10))

    for idx, col in enumerate(policy_cols):
        if col in data.columns:
            ax = axes[idx]

            # ì •ì±… ë³€ìˆ˜ì™€ ì „ë ¥ ìˆ˜ìš”ì˜ ê´€ê³„
            ax2 = ax.twinx()

            ax.plot(data.index[:1000], data[col].iloc[:1000],
                   color='blue', alpha=0.7, label=col)
            ax2.plot(data.index[:1000], data['demand_mw'].iloc[:1000],
                    color='red', alpha=0.5, label='ì „ë ¥ ìˆ˜ìš”')

            ax.set_xlabel('ì‹œê°„')
            ax.set_ylabel(col, color='blue')
            ax2.set_ylabel('ì „ë ¥ ìˆ˜ìš”', color='red')
            ax.tick_params(axis='y', labelcolor='blue')
            ax2.tick_params(axis='y', labelcolor='red')

            ax.set_title(f'{col}ì™€ ì „ë ¥ ìˆ˜ìš”ì˜ ê´€ê³„')
            ax.grid(True, alpha=0.3)

            # ìºí”Œì…˜ ì¶”ê°€
            caption_text = {
                'renewable_target': 'ì¬ìƒì—ë„ˆì§€ ëª©í‘œ ì¦ê°€ê°€ ì „ë ¥ ìˆ˜ìš”ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„',
                'subsidy_rate': 'ë³´ì¡°ê¸ˆ ë¹„ìœ¨ ë³€í™”ì™€ ì „ë ¥ ìˆ˜ìš”ì˜ ìƒê´€ê´€ê³„',
                'carbon_tax': 'íƒ„ì†Œì„¸ ì •ì±…ì´ ì „ë ¥ ì†Œë¹„ íŒ¨í„´ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼'
            }.get(col, '')

            if caption_text:
                ax.text(0.5, -0.15, caption_text,
                        ha='center', fontsize=9, style='italic', wrap=True, transform=ax.transAxes)

    plt.tight_layout()
    plt.savefig(os.path.join('output', 'policy_impact_analysis.png'), dpi=150, bbox_inches='tight')
    plt.show()

    print(f"âœ… ì •ì±… ì˜í–¥ ë¶„ì„ ì™„ë£Œ ({os.path.join('output', 'policy_impact_analysis.png')})")

def perform_statistical_analysis(data):
    """í†µê³„ì  ë¶„ì„ ìˆ˜í–‰"""
    print("\n" + "="*50)
    print("í†µê³„ì  ë¶„ì„")
    print("="*50)

    # ê¸°ë³¸ í†µê³„ëŸ‰
    print("\nğŸ“Š ê¸°ë³¸ í†µê³„ëŸ‰:")
    print("-" * 50)
    print(data.describe())

    # ìƒê´€ê´€ê³„ ë¶„ì„
    numeric_data = data.select_dtypes(include=[np.number])
    correlation_matrix = numeric_data.corr()

    # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    plt.figure(figsize=(12, 8))
    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)
    plt.title('ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„')

    # íˆíŠ¸ë§µ ìºí”Œì…˜
    plt.figtext(0.5, -0.02, 'ìƒê´€ê³„ìˆ˜: -1(ì™„ì „ ìŒì˜ ìƒê´€) ~ 0(ë¬´ê´€) ~ +1(ì™„ì „ ì–‘ì˜ ìƒê´€)\nê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ë“¤ì´ ì˜ˆì¸¡ì— ì¤‘ìš”',
                ha='center', fontsize=9, style='italic', wrap=True)

    plt.tight_layout()
    plt.savefig(os.path.join('output', 'correlation_heatmap.png'), dpi=150, bbox_inches='tight')
    plt.show()

    print(f"âœ… ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ìƒì„± ì™„ë£Œ ({os.path.join('output', 'correlation_heatmap.png')})")

    # ì£¼ìš” ìƒê´€ê´€ê³„ ì¶œë ¥
    print("\nğŸ“ˆ ì „ë ¥ ìˆ˜ìš”ì™€ì˜ ì£¼ìš” ìƒê´€ê´€ê³„:")
    print("-" * 50)
    demand_corr = correlation_matrix['demand_mw'].sort_values(ascending=False)
    for var, corr in demand_corr.items():
        if var != 'demand_mw' and abs(corr) > 0.3:
            print(f"{var:20s}: {corr:+.3f}")

def evaluate_loaded_models(models, data, scaler):
    """ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰"""
    print("\n" + "="*50)
    print("ì €ì¥ëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰")
    print("="*50)

    # ë°ì´í„° ì „ì²˜ë¦¬
    numeric_columns = data.select_dtypes(include=[np.number]).columns
    data_scaled = data.copy()
    data_scaled[numeric_columns] = scaler.transform(data[numeric_columns])

    # ì‹œí€€ìŠ¤ ìƒì„±
    X, y = create_sequences(data_scaled, sequence_length=24)

    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
    n_samples = len(X)
    train_size = int(0.7 * n_samples)
    val_size = int(0.15 * n_samples)
    X_test = X[train_size+val_size:]
    y_test = y[train_size+val_size:]

    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    # ëª¨ë¸ í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    print("-" * 50)

    results = {}
    for name, model in models.items():
        test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)
        results[name] = {'loss': test_loss, 'mae': test_mae}
        print(f"{name:10s} - Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}")

    # ì˜ˆì¸¡ ì‹œê°í™”
    visualize_predictions(models, X_test, y_test, scaler)

    return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ - ì‹¤ì „ ë¶„ì„")
    print("="*60)

    print("\nì‹¤í–‰í•  ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ëª¨ë¸ í•™ìŠµ ë° ì €ì¥")
    print("2. ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° í‰ê°€")
    print("3. ì •ì±… ì˜í–¥ ë¶„ì„ (ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)")
    print("4. í†µê³„ì  ë¶„ì„ (ë°ì´í„° ìƒê´€ê´€ê³„, ê¸°ë³¸ í†µê³„ëŸ‰, íˆíŠ¸ë§µ ìƒì„±)")

    while True:
        try:
            choice = input("\nì„ íƒ (1-4): ").strip()
            if choice in ['1', '2', '3', '4']:
                choice = int(choice)
                break
            else:
                print("ì˜¬ë°”ë¥¸ ì„ íƒì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” (1-4)")
        except:
            print("ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

    # ë°ì´í„° ë¡œë“œ
    data = load_and_prepare_data()

    if choice == 1:
        # ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        models, histories, results, scaler = train_and_evaluate_models(data, epochs=3, save=True)
        print("\nâœ… ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")

    elif choice == 2:
        # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ë° í‰ê°€
        try:
            models, scaler = load_models()
            results = evaluate_loaded_models(models, data, scaler)
            print("\nâœ… ì €ì¥ëœ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")
        except FileNotFoundError as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            print("ë¨¼ì € ì˜µì…˜ 1ì„ ì„ íƒí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•˜ì„¸ìš”.")

    elif choice == 3:
        # ì •ì±… ì˜í–¥ ë¶„ì„ (ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)
        try:
            models, scaler = load_models()
            analyze_policy_impact(data, models['LSTM'])
            print("\nâœ… ì •ì±… ì˜í–¥ ë¶„ì„ ì™„ë£Œ!")
        except FileNotFoundError as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            print("ë¨¼ì € ì˜µì…˜ 1ì„ ì„ íƒí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì €ì¥í•˜ì„¸ìš”.")

    elif choice == 4:
        # í†µê³„ì  ë¶„ì„
        perform_statistical_analysis(data)
        print("\nâœ… í†µê³„ì  ë¶„ì„ ì™„ë£Œ!")

    print("\n" + "="*60)
    print("ë¶„ì„ í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("="*60)

if __name__ == "__main__":
    main()