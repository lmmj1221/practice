# Example 1.3: Traditional Regression vs Machine Learning Performance Comparison
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# Load data from CSV file
data = pd.read_csv('1-1.csv')
print(f"Data loaded from 1-1.csv")
print(f"Shape: {data.shape}")
print(f"Columns: {list(data.columns)}\n")

# Separate features and target
feature_columns = ['budget', 'population', 'education_level', 'infrastructure', 'previous_performance']
X = data[feature_columns]
y = data['policy_outcome']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Traditional linear regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
lr_r2 = r2_score(y_test, lr_pred)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))

# AI-based Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_r2 = r2_score(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

# Compare results
print("=== Traditional vs AI-based Policy Analysis ===\n")
print("Linear Regression (Traditional):")
print(f"  RÂ² Score: {lr_r2:.4f}")
print(f"  RMSE: {lr_rmse:.2f}")
print(f"\nRandom Forest (AI-based):")
print(f"  RÂ² Score: {rf_r2:.4f}")
print(f"  RMSE: {rf_rmse:.2f}")
print(f"\nPerformance Improvement:")
print(f"  RÂ² Improvement: {(rf_r2 - lr_r2) / lr_r2 * 100:.1f}%")
print(f"  RMSE Reduction: {(lr_rmse - rf_rmse) / lr_rmse * 100:.1f}%")

# ê²°ê³¼ í•´ì„
print("\n" + "="*50)
print("ğŸ“Š ê²°ê³¼ í•´ì„")
print("="*50)

# RÂ² ì ìˆ˜ í•´ì„
if lr_r2 > 0.95 and rf_r2 > 0.95:
    print("\nâœ… ëª¨ë¸ ì„±ëŠ¥:")
    print(f"  â€¢ ë‘ ëª¨ë¸ ëª¨ë‘ ë§¤ìš° ë†’ì€ ì„¤ëª…ë ¥ì„ ë³´ì„ (RÂ² > 0.95)")
    print(f"  â€¢ ì„ í˜• íšŒê·€: {lr_r2:.1%}ì˜ ë¶„ì‚°ì„ ì„¤ëª…")
    print(f"  â€¢ ëœë¤ í¬ë ˆìŠ¤íŠ¸: {rf_r2:.1%}ì˜ ë¶„ì‚°ì„ ì„¤ëª…")
elif rf_r2 > lr_r2:
    print("\nâœ… ëª¨ë¸ ì„±ëŠ¥:")
    print(f"  â€¢ AI ëª¨ë¸ì´ ì „í†µì  ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥")
    print(f"  â€¢ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ë” ì˜ í¬ì°©í•¨")
else:
    print("\nâœ… ëª¨ë¸ ì„±ëŠ¥:")
    print(f"  â€¢ ì„ í˜• íšŒê·€ê°€ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„")
    print(f"  â€¢ ë°ì´í„°ê°€ ì„ í˜• ê´€ê³„ì— ê°€ê¹Œì›€ì„ ì‹œì‚¬")

# RMSE í•´ì„
print(f"\nğŸ“ˆ ì˜ˆì¸¡ ì •í™•ë„ (RMSE):")
print(f"  â€¢ ì„ í˜• íšŒê·€ì˜ í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨: Â±{lr_rmse:.1f}")
print(f"  â€¢ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ í‰ê·  ì˜ˆì¸¡ ì˜¤ì°¨: Â±{rf_rmse:.1f}")
if rf_rmse < lr_rmse:
    print(f"  â€¢ AI ëª¨ë¸ì´ {lr_rmse - rf_rmse:.1f} ë§Œí¼ ë” ì •í™•í•œ ì˜ˆì¸¡")
else:
    print(f"  â€¢ ì„ í˜• ëª¨ë¸ì´ {rf_rmse - lr_rmse:.1f} ë§Œí¼ ë” ì •í™•í•œ ì˜ˆì¸¡")

# íŠ¹ì§• ì¤‘ìš”ë„ ë¶„ì„
print(f"\nğŸ” ì£¼ìš” ì •ì±… ìš”ì¸ (Feature Importance):")
feature_importance_sorted = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

for idx, row in feature_importance_sorted.iterrows():
    feature_name_kr = {
        'budget': 'ì˜ˆì‚°',
        'population': 'ì¸êµ¬',
        'education_level': 'êµìœ¡ ìˆ˜ì¤€',
        'infrastructure': 'ì¸í”„ë¼',
        'previous_performance': 'ì´ì „ ì„±ê³¼'
    }
    print(f"  {idx+1}. {feature_name_kr.get(row['feature'], row['feature'])}: {row['importance']:.1%}")

# ì¢…í•© í‰ê°€
print("\nğŸ’¡ ì¢…í•© í‰ê°€:")
if abs(rf_r2 - lr_r2) < 0.01:
    print("  â€¢ ì´ ë°ì´í„°ì…‹ì—ì„œëŠ” ë‘ ëª¨ë¸ì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ë¯¸ë¯¸í•¨")
    print("  â€¢ ë‹¨ìˆœí•œ ì„ í˜• ëª¨ë¸ë¡œë„ ì¶©ë¶„í•œ ì˜ˆì¸¡ë ¥ì„ ë³´ì„")
    print("  â€¢ ê³„ì‚° íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ë©´ ì„ í˜• íšŒê·€ê°€ ë” ì í•©í•  ìˆ˜ ìˆìŒ")
elif rf_r2 > lr_r2:
    print("  â€¢ AI ê¸°ë°˜ ëª¨ë¸ì´ ë³µì¡í•œ íŒ¨í„´ì„ ë” ì˜ í•™ìŠµí•¨")
    print("  â€¢ ë¹„ì„ í˜• ê´€ê³„ì™€ ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©")
    print("  â€¢ ì •ì±… ì˜ˆì¸¡ì— ë¨¸ì‹ ëŸ¬ë‹ í™œìš©ì˜ ì¥ì ì„ ë³´ì—¬ì¤Œ")
else:
    print("  â€¢ ì „í†µì  í†µê³„ ëª¨ë¸ì´ ì´ ê²½ìš° ë” ì í•©í•¨")
    print("  â€¢ ë°ì´í„°ì˜ ì„ í˜•ì„±ì´ ê°•í•˜ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ë§ì„ ê°€ëŠ¥ì„±")
    print("  â€¢ ê³¼ì í•© ìœ„í—˜ ì—†ì´ ì•ˆì •ì ì¸ ì˜ˆì¸¡ ì œê³µ")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Prediction vs actual comparison
ax1 = axes[0]
ax1.scatter(y_test, lr_pred, alpha=0.5, label='Linear Regression', color='blue')
ax1.scatter(y_test, rf_pred, alpha=0.5, label='Random Forest', color='red')
ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
ax1.set_xlabel('Actual Policy Outcome')
ax1.set_ylabel('Predicted Policy Outcome')
# ax1.set_title('Prediction Accuracy Comparison')  # Removed per guidelines
ax1.legend()
ax1.grid(True, alpha=0.3)

# Feature importance (Random Forest)
ax2 = axes[1]
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': rf_model.feature_importances_
}).sort_values('importance', ascending=False)

bars = ax2.bar(range(len(feature_importance)), feature_importance['importance'])
ax2.set_xticks(range(len(feature_importance)))
ax2.set_xticklabels(feature_importance['feature'], rotation=45, ha='right')
ax2.set_ylabel('Feature Importance')
# ax2.set_title('AI Model Feature Importance')  # Removed per guidelines
ax2.grid(True, alpha=0.3)

# Apply color gradient
colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(bars)))
for bar, color in zip(bars, colors):
    bar.set_color(color)

plt.tight_layout()
plt.savefig('traditional_vs_ai_comparison.png', dpi=150, bbox_inches='tight')
plt.show()