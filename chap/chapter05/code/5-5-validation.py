#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ
êµì°¨ê²€ì¦ ê¸°ë°˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ê²€ì¦
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import (
    train_test_split, cross_val_score, cross_validate,
    TimeSeriesSplit, StratifiedKFold, KFold
)
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    make_scorer, mean_absolute_percentage_error
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10

class ModelValidationSystem:
    """ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ í´ë˜ìŠ¤"""

    def __init__(self, cv_folds=5, random_state=42):
        """
        ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Parameters:
        cv_folds (int): êµì°¨ê²€ì¦ í´ë“œ ìˆ˜
        random_state (int): ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
        """
        self.cv_folds = cv_folds
        self.random_state = random_state
        self.models = {}
        self.validation_results = {}
        self.cv_results = {}
        self.scaler = StandardScaler()

    def generate_validation_data(self, n_samples=1200, n_features=6, scenario='mixed'):
        """
        ëª¨ë¸ ê²€ì¦ì„ ìœ„í•œ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„±
        â€» ë³¸ ë°ì´í„°ëŠ” êµìœ¡ ëª©ì ì˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤

        Parameters:
        n_samples (int): ìƒ˜í”Œ ìˆ˜
        n_features (int): íŠ¹ì„± ìˆ˜
        scenario (str): ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤ ('linear', 'nonlinear', 'mixed')

        Returns:
        tuple: (X, y, feature_names) íŠ¹ì„±, íƒ€ê²Ÿ, íŠ¹ì„±ëª…
        """
        np.random.seed(self.random_state)

        feature_names = [
            'ì •ì±…íˆ¬ìê·œëª¨', 'ê²½ì œì—¬ê±´ì§€ìˆ˜', 'ì‚¬íšŒì¸í”„ë¼ì§€ìˆ˜',
            'ê¸°ìˆ í˜ì‹ ì§€ìˆ˜', 'ì¸ì ìì›ì§€ìˆ˜', 'í™˜ê²½ì§€ì†ì„±ì§€ìˆ˜'
        ]

        # ê¸°ë³¸ íŠ¹ì„± ìƒì„±
        X = np.random.randn(n_samples, n_features)

        # ì‹œë‚˜ë¦¬ì˜¤ë³„ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        if scenario == 'linear':
            # ì„ í˜• ê´€ê³„
            weights = np.array([2.5, 1.8, 1.2, 0.9, 1.5, 0.7])
            y = X @ weights + 0.3 * np.random.randn(n_samples)

        elif scenario == 'nonlinear':
            # ë¹„ì„ í˜• ê´€ê³„
            y = (2.0 * X[:, 0] +
                 1.5 * np.power(X[:, 1], 2) +
                 1.0 * np.sin(3 * X[:, 2]) +
                 0.8 * np.exp(0.5 * X[:, 3]) +
                 0.6 * np.log(np.abs(X[:, 4]) + 1) +
                 0.4 * X[:, 5] +
                 0.5 * np.random.randn(n_samples))

        elif scenario == 'mixed':
            # í˜¼í•© ê´€ê³„ (ì„ í˜• + ë¹„ì„ í˜• + ìƒí˜¸ì‘ìš©)
            linear_part = 1.5 * X[:, 0] + 1.0 * X[:, 1]
            nonlinear_part = 0.8 * np.power(X[:, 2], 2) + 0.6 * np.sin(2 * X[:, 3])
            interaction_part = 0.4 * X[:, 4] * X[:, 5]
            noise = 0.4 * np.random.randn(n_samples)

            y = linear_part + nonlinear_part + interaction_part + noise

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤: {scenario}")

        # DataFrame ìƒì„±
        X_df = pd.DataFrame(X, columns=feature_names)

        print(f"âœ… ëª¨ë¸ ê²€ì¦ìš© ë°ì´í„° ìƒì„± ì™„ë£Œ ({scenario} ì‹œë‚˜ë¦¬ì˜¤)")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {n_samples}")
        print(f"   - íŠ¹ì„± ìˆ˜: {n_features}")
        print(f"   - íƒ€ê²Ÿ ë²”ìœ„: [{y.min():.2f}, {y.max():.2f}]")

        return X_df, y, feature_names

    def create_model_suite(self):
        """
        ê²€ì¦í•  ëª¨ë¸ ëª¨ìŒ ìƒì„±

        Returns:
        dict: ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
        """
        # ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ ëª¨ë¸ë“¤ ìƒì„±
        self.models = {
            # ì„ í˜• ëª¨ë¸
            'linear_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('regressor', LinearRegression())
            ]),

            'ridge_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('regressor', Ridge(alpha=1.0, random_state=self.random_state))
            ]),

            'lasso_regression': Pipeline([
                ('scaler', StandardScaler()),
                ('regressor', Lasso(alpha=0.1, random_state=self.random_state))
            ]),

            # íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=8,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=self.random_state,
                n_jobs=-1
            ),

            'gradient_boosting': GradientBoostingRegressor(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                subsample=0.8,
                random_state=self.random_state
            ),

            # ë¹„ì„ í˜• ëª¨ë¸
            'svr_rbf': Pipeline([
                ('scaler', StandardScaler()),
                ('regressor', SVR(kernel='rbf', C=1.0, gamma='scale'))
            ])
        }

        print(f"âœ… ëª¨ë¸ ëª¨ìŒ ìƒì„± ì™„ë£Œ: {len(self.models)}ê°œ ëª¨ë¸")
        for name in self.models.keys():
            print(f"   - {name}")

        return self.models

    def cross_validation_analysis(self, X, y, cv_type='kfold'):
        """
        êµì°¨ê²€ì¦ ë¶„ì„ ìˆ˜í–‰

        Parameters:
        X (DataFrame): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜
        cv_type (str): êµì°¨ê²€ì¦ íƒ€ì… ('kfold', 'stratified', 'timeseries')

        Returns:
        dict: êµì°¨ê²€ì¦ ê²°ê³¼
        """
        print(f"\nğŸ”„ êµì°¨ê²€ì¦ ë¶„ì„ ì‹œì‘ ({cv_type}, {self.cv_folds}-fold)")

        # êµì°¨ê²€ì¦ ì „ëµ ì„ íƒ
        if cv_type == 'kfold':
            cv_strategy = KFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        elif cv_type == 'timeseries':
            cv_strategy = TimeSeriesSplit(n_splits=self.cv_folds)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” êµì°¨ê²€ì¦ íƒ€ì…: {cv_type}")

        # í‰ê°€ ì§€í‘œ ì •ì˜
        scoring = {
            'mse': make_scorer(mean_squared_error),
            'mae': make_scorer(mean_absolute_error),
            'r2': 'r2',
            'mape': make_scorer(mean_absolute_percentage_error, greater_is_better=False)
        }

        cv_results = {}

        for name, model in self.models.items():
            print(f"   ğŸ”„ {name} êµì°¨ê²€ì¦ ì¤‘...")

            # êµì°¨ê²€ì¦ ìˆ˜í–‰
            cv_scores = cross_validate(
                model, X, y,
                cv=cv_strategy,
                scoring=scoring,
                return_train_score=True,
                n_jobs=-1
            )

            # ê²°ê³¼ ì •ë¦¬
            cv_results[name] = {
                'test_mse': cv_scores['test_mse'],
                'test_mae': cv_scores['test_mae'],
                'test_r2': cv_scores['test_r2'],
                'test_mape': -cv_scores['test_mape'],  # ì›ë˜ ë¶€í˜¸ë¡œ ë³µì›
                'train_mse': cv_scores['train_mse'],
                'train_mae': cv_scores['train_mae'],
                'train_r2': cv_scores['train_r2'],
                'train_mape': -cv_scores['train_mape'],
                'fit_time': cv_scores['fit_time'],
                'score_time': cv_scores['score_time']
            }

            # ìš”ì•½ í†µê³„ ì¶œë ¥
            test_r2_mean = cv_results[name]['test_r2'].mean()
            test_r2_std = cv_results[name]['test_r2'].std()

            print(f"      âœ… ì™„ë£Œ - RÂ²: {test_r2_mean:.4f} (Â±{test_r2_std:.4f})")

        self.cv_results = cv_results

        print("âœ… ëª¨ë“  ëª¨ë¸ êµì°¨ê²€ì¦ ì™„ë£Œ!")

        return cv_results

    def holdout_validation(self, X, y, test_size=0.2):
        """
        í™€ë“œì•„ì›ƒ ê²€ì¦ ìˆ˜í–‰

        Parameters:
        X (DataFrame): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜
        test_size (float): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨

        Returns:
        dict: í™€ë“œì•„ì›ƒ ê²€ì¦ ê²°ê³¼
        """
        print(f"\nğŸ“Š í™€ë“œì•„ì›ƒ ê²€ì¦ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ë¹„ìœ¨: {test_size})")

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state
        )

        holdout_results = {}

        for name, model in self.models.items():
            print(f"   ğŸ”„ {name} í™€ë“œì•„ì›ƒ ê²€ì¦ ì¤‘...")

            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train, y_train)

            # ì˜ˆì¸¡
            train_pred = model.predict(X_train)
            test_pred = model.predict(X_test)

            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            holdout_results[name] = {
                'train_mse': mean_squared_error(y_train, train_pred),
                'train_mae': mean_absolute_error(y_train, train_pred),
                'train_r2': r2_score(y_train, train_pred),
                'test_mse': mean_squared_error(y_test, test_pred),
                'test_mae': mean_absolute_error(y_test, test_pred),
                'test_r2': r2_score(y_test, test_pred),
                'overfit_score': r2_score(y_train, train_pred) - r2_score(y_test, test_pred),
                'predictions': test_pred,
                'actual': y_test
            }

            test_r2 = holdout_results[name]['test_r2']
            overfit = holdout_results[name]['overfit_score']

            print(f"      âœ… ì™„ë£Œ - RÂ²: {test_r2:.4f}, ê³¼ì í•©: {overfit:.4f}")

        self.validation_results = holdout_results

        print("âœ… ëª¨ë“  ëª¨ë¸ í™€ë“œì•„ì›ƒ ê²€ì¦ ì™„ë£Œ!")

        return holdout_results

    def analyze_model_stability(self, X, y, n_runs=10):
        """
        ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„ (ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰)

        Parameters:
        X (DataFrame): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜
        n_runs (int): ì‹¤í–‰ íšŸìˆ˜

        Returns:
        dict: ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼
        """
        print(f"\nğŸ¯ ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„ ì‹œì‘ ({n_runs}íšŒ ì‹¤í–‰)")

        stability_results = {}

        for name in self.models.keys():
            stability_results[name] = {
                'test_r2_scores': [],
                'test_mse_scores': [],
                'train_r2_scores': [],
                'train_mse_scores': []
            }

        for run in range(n_runs):
            print(f"   ğŸ”„ ì‹¤í–‰ {run+1}/{n_runs}")

            # ë§¤ë²ˆ ë‹¤ë¥¸ ë¶„í• 
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=self.random_state + run
            )

            for name, model in self.models.items():
                # ëª¨ë¸ ë³µì‚¬ ë° í•™ìŠµ (Pipelineê³¼ ì¼ë°˜ ëª¨ë¸ êµ¬ë¶„)
                if hasattr(model, 'steps'):  # Pipeline ê°ì²´ì¸ ê²½ìš°
                    from sklearn.base import clone
                    model_copy = clone(model)
                else:
                    model_copy = model.__class__(**model.get_params())
                model_copy.fit(X_train, y_train)

                # ì˜ˆì¸¡ ë° í‰ê°€
                train_pred = model_copy.predict(X_train)
                test_pred = model_copy.predict(X_test)

                train_r2 = r2_score(y_train, train_pred)
                test_r2 = r2_score(y_test, test_pred)
                train_mse = mean_squared_error(y_train, train_pred)
                test_mse = mean_squared_error(y_test, test_pred)

                stability_results[name]['train_r2_scores'].append(train_r2)
                stability_results[name]['test_r2_scores'].append(test_r2)
                stability_results[name]['train_mse_scores'].append(train_mse)
                stability_results[name]['test_mse_scores'].append(test_mse)

        # ì•ˆì •ì„± ì§€í‘œ ê³„ì‚°
        for name in self.models.keys():
            test_r2_scores = np.array(stability_results[name]['test_r2_scores'])

            stability_results[name]['mean_test_r2'] = test_r2_scores.mean()
            stability_results[name]['std_test_r2'] = test_r2_scores.std()
            stability_results[name]['cv_test_r2'] = test_r2_scores.std() / test_r2_scores.mean()  # ë³€ë™ê³„ìˆ˜

            print(f"   ğŸ“Š {name}: RÂ² = {test_r2_scores.mean():.4f} Â± {test_r2_scores.std():.4f}")

        print("âœ… ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„ ì™„ë£Œ!")

        return stability_results

    def print_validation_summary(self):
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.cv_results and not self.validation_results:
            print("âš ï¸ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê²€ì¦ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
            return

        print("\n" + "="*80)
        print("ğŸ“Š ëª¨ë¸ ê²€ì¦ ê²°ê³¼ ì¢…í•© ìš”ì•½")
        print("="*80)

        # êµì°¨ê²€ì¦ ê²°ê³¼
        if self.cv_results:
            print("\nğŸ”„ êµì°¨ê²€ì¦ ê²°ê³¼:")
            print("-" * 60)
            for name, results in self.cv_results.items():
                test_r2 = results['test_r2']
                test_mse = results['test_mse']

                print(f"\nğŸ”¹ {name.upper()}:")
                print(f"  RÂ² (í…ŒìŠ¤íŠ¸):  {test_r2.mean():.4f} Â± {test_r2.std():.4f}")
                print(f"  MSE (í…ŒìŠ¤íŠ¸): {test_mse.mean():.4f} Â± {test_mse.std():.4f}")
                print(f"  í‰ê·  í•™ìŠµì‹œê°„: {results['fit_time'].mean():.3f}ì´ˆ")

        # í™€ë“œì•„ì›ƒ ê²€ì¦ ê²°ê³¼
        if self.validation_results:
            print("\nğŸ“Š í™€ë“œì•„ì›ƒ ê²€ì¦ ê²°ê³¼:")
            print("-" * 60)
            for name, results in self.validation_results.items():
                print(f"\nğŸ”¹ {name.upper()}:")
                print(f"  RÂ² (í•™ìŠµ):    {results['train_r2']:.4f}")
                print(f"  RÂ² (í…ŒìŠ¤íŠ¸):  {results['test_r2']:.4f}")
                print(f"  ê³¼ì í•© ì§€í‘œ:   {results['overfit_score']:.4f}")

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„
        if self.cv_results:
            best_model_cv = max(self.cv_results.keys(),
                               key=lambda x: self.cv_results[x]['test_r2'].mean())
            print(f"\nğŸ† êµì°¨ê²€ì¦ ìµœê³  ì„±ëŠ¥: {best_model_cv}")

        if self.validation_results:
            best_model_holdout = max(self.validation_results.keys(),
                                   key=lambda x: self.validation_results[x]['test_r2'])
            print(f"ğŸ† í™€ë“œì•„ì›ƒ ìµœê³  ì„±ëŠ¥: {best_model_holdout}")

    def plot_validation_results(self, save_path='practice/chapter05/outputs/validation_results.png'):
        """
        ê²€ì¦ ê²°ê³¼ ì‹œê°í™”

        Parameters:
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        if not self.cv_results and not self.validation_results:
            print("âš ï¸ ì‹œê°í™”í•  ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. êµì°¨ê²€ì¦ RÂ² ë°•ìŠ¤í”Œë¡¯
        if self.cv_results:
            ax1 = axes[0, 0]
            cv_data = []
            cv_labels = []

            for name, results in self.cv_results.items():
                cv_data.append(results['test_r2'])
                cv_labels.append(name)

            bp1 = ax1.boxplot(cv_data, labels=cv_labels, patch_artist=True)
            ax1.set_title('êµì°¨ê²€ì¦ RÂ² ë¶„í¬')
            ax1.set_ylabel('RÂ² ì ìˆ˜')
            ax1.tick_params(axis='x', rotation=45)

            # ë°•ìŠ¤í”Œë¡¯ ìƒ‰ìƒ ì„¤ì •
            colors = plt.cm.Set3(np.linspace(0, 1, len(cv_data)))
            for patch, color in zip(bp1['boxes'], colors):
                patch.set_facecolor(color)

        # 2. í™€ë“œì•„ì›ƒ ê²€ì¦ í•™ìŠµ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
        if self.validation_results:
            ax2 = axes[0, 1]
            models = list(self.validation_results.keys())
            train_r2 = [self.validation_results[model]['train_r2'] for model in models]
            test_r2 = [self.validation_results[model]['test_r2'] for model in models]

            x_pos = np.arange(len(models))
            width = 0.35

            ax2.bar(x_pos - width/2, train_r2, width, label='í•™ìŠµ RÂ²', alpha=0.8)
            ax2.bar(x_pos + width/2, test_r2, width, label='í…ŒìŠ¤íŠ¸ RÂ²', alpha=0.8)

            ax2.set_title('í•™ìŠµ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥')
            ax2.set_ylabel('RÂ² ì ìˆ˜')
            ax2.set_xticks(x_pos)
            ax2.set_xticklabels(models, rotation=45, ha='right')
            ax2.legend()

        # 3. ê³¼ì í•© ë¶„ì„
        if self.validation_results:
            ax3 = axes[1, 0]
            models = list(self.validation_results.keys())
            overfit_scores = [self.validation_results[model]['overfit_score'] for model in models]

            bars = ax3.bar(models, overfit_scores, alpha=0.7)
            ax3.set_title('ê³¼ì í•© ë¶„ì„')
            ax3.set_ylabel('ê³¼ì í•© ì ìˆ˜ (í•™ìŠµRÂ² - í…ŒìŠ¤íŠ¸RÂ²)')
            ax3.tick_params(axis='x', rotation=45)
            ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7)

            # ìƒ‰ìƒ ì„¤ì • (ê³¼ì í•© ì •ë„ì— ë”°ë¼)
            for bar, score in zip(bars, overfit_scores):
                if score > 0.1:
                    bar.set_color('red')
                elif score > 0.05:
                    bar.set_color('orange')
                else:
                    bar.set_color('green')

        # 4. ëª¨ë¸ ì‹¤í–‰ ì‹œê°„ ë¹„êµ (êµì°¨ê²€ì¦ ê¸°ì¤€)
        if self.cv_results:
            ax4 = axes[1, 1]
            models = list(self.cv_results.keys())
            fit_times = [self.cv_results[model]['fit_time'].mean() for model in models]

            bars = ax4.bar(models, fit_times, alpha=0.7, color='skyblue')
            ax4.set_title('í‰ê·  í•™ìŠµ ì‹œê°„')
            ax4.set_ylabel('ì‹œê°„ (ì´ˆ)')
            ax4.tick_params(axis='x', rotation=45)

            # ì‹œê°„ ê°’ í‘œì‹œ
            for bar, time in zip(bars, fit_times):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{time:.3f}s', ha='center', va='bottom', fontsize=8)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ ê²€ì¦ ê²°ê³¼ ì‹œê°í™” ì €ì¥: {save_path}")

    def generate_validation_report(self, save_path='practice/chapter05/outputs/validation_report.txt'):
        """
        ìƒì„¸ ê²€ì¦ ë³´ê³ ì„œ ìƒì„±

        Parameters:
        save_path (str): ë³´ê³ ì„œ ì €ì¥ ê²½ë¡œ
        """
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì¢…í•© ë³´ê³ ì„œ\n")
            f.write("="*80 + "\n\n")

            # êµì°¨ê²€ì¦ ê²°ê³¼ ìƒì„¸
            if self.cv_results:
                f.write("1. êµì°¨ê²€ì¦ ê²°ê³¼ ìƒì„¸\n")
                f.write("-" * 40 + "\n")

                for name, results in self.cv_results.items():
                    f.write(f"\n[{name.upper()}]\n")
                    f.write(f"  í…ŒìŠ¤íŠ¸ RÂ²:   {results['test_r2'].mean():.4f} Â± {results['test_r2'].std():.4f}\n")
                    f.write(f"  í…ŒìŠ¤íŠ¸ MSE:  {results['test_mse'].mean():.4f} Â± {results['test_mse'].std():.4f}\n")
                    f.write(f"  í…ŒìŠ¤íŠ¸ MAE:  {results['test_mae'].mean():.4f} Â± {results['test_mae'].std():.4f}\n")
                    f.write(f"  í‰ê·  í•™ìŠµì‹œê°„: {results['fit_time'].mean():.3f}ì´ˆ\n")

            # í™€ë“œì•„ì›ƒ ê²€ì¦ ê²°ê³¼ ìƒì„¸
            if self.validation_results:
                f.write("\n\n2. í™€ë“œì•„ì›ƒ ê²€ì¦ ê²°ê³¼ ìƒì„¸\n")
                f.write("-" * 40 + "\n")

                for name, results in self.validation_results.items():
                    f.write(f"\n[{name.upper()}]\n")
                    f.write(f"  í•™ìŠµ RÂ²:     {results['train_r2']:.4f}\n")
                    f.write(f"  í…ŒìŠ¤íŠ¸ RÂ²:   {results['test_r2']:.4f}\n")
                    f.write(f"  í•™ìŠµ MSE:    {results['train_mse']:.4f}\n")
                    f.write(f"  í…ŒìŠ¤íŠ¸ MSE:  {results['test_mse']:.4f}\n")
                    f.write(f"  ê³¼ì í•© ì§€í‘œ: {results['overfit_score']:.4f}\n")

            # ê¶Œì¥ì‚¬í•­
            f.write("\n\n3. ëª¨ë¸ ì„ íƒ ê¶Œì¥ì‚¬í•­\n")
            f.write("-" * 40 + "\n")

            if self.cv_results:
                best_cv = max(self.cv_results.keys(),
                             key=lambda x: self.cv_results[x]['test_r2'].mean())
                f.write(f"â€¢ êµì°¨ê²€ì¦ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥: {best_cv}\n")

            if self.validation_results:
                best_holdout = max(self.validation_results.keys(),
                                 key=lambda x: self.validation_results[x]['test_r2'])
                f.write(f"â€¢ í™€ë“œì•„ì›ƒ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥: {best_holdout}\n")

                # ê³¼ì í•©ì´ ì ì€ ëª¨ë¸ ì¶”ì²œ
                stable_models = [name for name, results in self.validation_results.items()
                               if results['overfit_score'] < 0.05]
                if stable_models:
                    f.write(f"â€¢ ì•ˆì •ì„±ì´ ìš°ìˆ˜í•œ ëª¨ë¸: {', '.join(stable_models)}\n")

        print(f"ğŸ“„ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {save_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)

    # ê²€ì¦ ì‹œìŠ¤í…œ ê°ì²´ ìƒì„±
    validator = ModelValidationSystem(cv_folds=5)

    # 1. ê²€ì¦ìš© ë°ì´í„° ìƒì„± (ì—¬ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤)
    scenarios = ['linear', 'nonlinear', 'mixed']

    for scenario in scenarios:
        print(f"\nğŸ“‹ {scenario.upper()} ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦")
        print("="*50)

        # ë°ì´í„° ìƒì„±
        X, y, feature_names = validator.generate_validation_data(
            n_samples=1200, n_features=6, scenario=scenario
        )

        # 2. ëª¨ë¸ ëª¨ìŒ ìƒì„±
        print("\nğŸ¤– ëª¨ë¸ ëª¨ìŒ ìƒì„±")
        validator.create_model_suite()

        # 3. êµì°¨ê²€ì¦ ë¶„ì„
        print("\nğŸ”„ êµì°¨ê²€ì¦ ë¶„ì„")
        cv_results = validator.cross_validation_analysis(X, y, cv_type='kfold')

        # 4. í™€ë“œì•„ì›ƒ ê²€ì¦
        print("\nğŸ“Š í™€ë“œì•„ì›ƒ ê²€ì¦")
        holdout_results = validator.holdout_validation(X, y, test_size=0.2)

        # 5. ì•ˆì •ì„± ë¶„ì„
        print("\nğŸ¯ ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„")
        stability_results = validator.analyze_model_stability(X, y, n_runs=5)

        # 6. ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        validator.print_validation_summary()

        # 7. ì‹œê°í™” (ë§ˆì§€ë§‰ ì‹œë‚˜ë¦¬ì˜¤ë§Œ)
        if scenario == scenarios[-1]:
            print("\nğŸ“ˆ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”")
            validator.plot_validation_results()

            # 8. ë³´ê³ ì„œ ìƒì„±
            print("\nğŸ“„ ê²€ì¦ ë³´ê³ ì„œ ìƒì„±")
            validator.generate_validation_report()

    # 9. ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("="*60)

    print(f"ğŸ“Š ê²€ì¦ ì™„ë£Œ:")
    print(f"   - ì‹œë‚˜ë¦¬ì˜¤: {len(scenarios)}ê°œ")
    print(f"   - ëª¨ë¸: {len(validator.models)}ê°œ")
    print(f"   - êµì°¨ê²€ì¦: {validator.cv_folds}-fold")

    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print("   - practice/chapter05/outputs/validation_results.png")
    print("   - practice/chapter05/outputs/validation_report.txt")

    print("\nâœ… ëª¨ë“  ëª¨ë¸ ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()