#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ëª¨ë¸ êµ¬í˜„ (ê°œì„ ëœ ë²„ì „)
ì •ì±… ì‹œê³„ì—´ ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ í†µí•© ëª¨ë¸
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, LayerNormalization, MultiHeadAttention, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.unicode_minus'] = False

class LSTMTransformerHybrid:
    """
    LSTMê³¼ Transformerë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸
    """

    def __init__(self, sequence_length=30, lstm_units=64, num_heads=4,
                 transformer_dim=64, dropout_rate=0.1):
        """
        ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            sequence_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
            lstm_units: LSTM ìœ ë‹› ìˆ˜
            num_heads: Multi-Head Attention í—¤ë“œ ìˆ˜
            transformer_dim: Transformer ì°¨ì›
            dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        """
        self.sequence_length = sequence_length
        self.lstm_units = lstm_units
        self.num_heads = num_heads
        self.transformer_dim = transformer_dim
        self.dropout_rate = dropout_rate
        self.model = None
        self.scaler = MinMaxScaler()

    def build_model(self, input_features):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•

        Args:
            input_features: ì…ë ¥ íŠ¹ì„± ìˆ˜
        """
        # ì…ë ¥ ë ˆì´ì–´
        inputs = Input(shape=(self.sequence_length, input_features))

        # LSTM ë ˆì´ì–´ - ìˆœì°¨ì  íŒ¨í„´ í•™ìŠµ
        lstm_out = LSTM(self.lstm_units, return_sequences=True,
                       dropout=self.dropout_rate, recurrent_dropout=self.dropout_rate)(inputs)
        lstm_out = LayerNormalization()(lstm_out)

        # Transformer ë ˆì´ì–´ - ì „ì—­ì  ì˜ì¡´ì„± í•™ìŠµ
        attention_out = MultiHeadAttention(
            num_heads=self.num_heads,
            key_dim=self.transformer_dim
        )(lstm_out, lstm_out)

        # ì”ì°¨ ì—°ê²°ê³¼ ì •ê·œí™”
        transformer_out = LayerNormalization()(lstm_out + attention_out)
        transformer_out = Dropout(self.dropout_rate)(transformer_out)

        # ê¸€ë¡œë²Œ í‰ê·  í’€ë§
        pooled = tf.keras.layers.GlobalAveragePooling1D()(transformer_out)

        # ì™„ì „ì—°ê²° ë ˆì´ì–´
        dense1 = Dense(self.transformer_dim, activation='relu')(pooled)
        dense1 = Dropout(self.dropout_rate)(dense1)
        dense2 = Dense(32, activation='relu')(dense1)
        outputs = Dense(1, activation='linear')(dense2)

        # ëª¨ë¸ ì»´íŒŒì¼
        self.model = Model(inputs=inputs, outputs=outputs)
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        return self.model

    def create_sequences(self, data, target_col):
        """
        ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜

        Args:
            data: ì…ë ¥ ë°ì´í„°í”„ë ˆì„
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
            X, y: ì…ë ¥ ì‹œí€€ìŠ¤ì™€ íƒ€ê²Ÿ ë°°ì—´
        """
        X, y = [], []

        for i in range(len(data) - self.sequence_length):
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ëª¨ë“  íŠ¹ì„±)
            X.append(data[i:(i + self.sequence_length)].values)
            # íƒ€ê²Ÿ ê°’ (ë‹¤ìŒ ì‹œì ì˜ íƒ€ê²Ÿ ë³€ìˆ˜)
            y.append(data[target_col].iloc[i + self.sequence_length])

        return np.array(X), np.array(y)

    def train(self, train_data, target_col, validation_split=0.2, epochs=50):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            train_data: í•™ìŠµ ë°ì´í„°
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…
            validation_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
            epochs: ì—í¬í¬ ìˆ˜
        """
        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        print("=" * 60)

        # ë°ì´í„° ì •ê·œí™”
        scaled_data = pd.DataFrame(
            self.scaler.fit_transform(train_data),
            columns=train_data.columns,
            index=train_data.index
        )

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self.create_sequences(scaled_data, target_col)

        print(f"ğŸ“‹ ë°ì´í„° ì •ë³´:")
        print(f"   - ì‹œí€€ìŠ¤ ê°œìˆ˜: {len(X)}")
        print(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {self.sequence_length}")
        print(f"   - íŠ¹ì„± ìˆ˜: {X.shape[2]}")

        # ëª¨ë¸ êµ¬ì¶•
        if self.model is None:
            self.build_model(X.shape[2])

        print(f"\nğŸ¤– ëª¨ë¸ êµ¬ì¡°:")
        print(f"   - LSTM ìœ ë‹›: {self.lstm_units}")
        print(f"   - Attention í—¤ë“œ: {self.num_heads}")
        print(f"   - Transformer ì°¨ì›: {self.transformer_dim}")

        # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )

        # ëª¨ë¸ í•™ìŠµ
        print(f"\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        history = self.model.fit(
            X, y,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=1
        )

        print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
        return history

    def predict(self, test_data, target_col):
        """
        ì˜ˆì¸¡ ìˆ˜í–‰

        Args:
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            target_col: íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
            predictions: ì˜ˆì¸¡ ê²°ê³¼
        """
        # ë°ì´í„° ì •ê·œí™” (í•™ìŠµì‹œ ì‚¬ìš©í•œ ìŠ¤ì¼€ì¼ëŸ¬ í™œìš©)
        scaled_data = pd.DataFrame(
            self.scaler.transform(test_data),
            columns=test_data.columns,
            index=test_data.index
        )

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y_true = self.create_sequences(scaled_data, target_col)

        # ì˜ˆì¸¡
        predictions_scaled = self.model.predict(X, verbose=0)

        # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³µì›
        target_idx = test_data.columns.get_loc(target_col)
        predictions = self.scaler.inverse_transform(
            np.column_stack([
                np.zeros((len(predictions_scaled), target_idx)),
                predictions_scaled.flatten(),
                np.zeros((len(predictions_scaled), len(test_data.columns) - target_idx - 1))
            ])
        )[:, target_idx]

        # ì‹¤ì œ ê°’ë„ ë³µì›
        y_true_original = test_data[target_col].iloc[self.sequence_length:].values

        return predictions, y_true_original

    def evaluate_performance(self, y_true, y_pred):
        """
        ëª¨ë¸ ì„±ëŠ¥ í‰ê°€

        Args:
            y_true: ì‹¤ì œ ê°’
            y_pred: ì˜ˆì¸¡ ê°’

        Returns:
            metrics: ì„±ëŠ¥ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
        """
        mse = mean_squared_error(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        metrics = {
            'MSE': mse,
            'RMSE': np.sqrt(mse),
            'MAE': mae,
            'RÂ²': r2
        }

        return metrics

def generate_predictable_policy_data(n_samples=1000):
    """
    ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì •ì±… ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ê³ ì„±ëŠ¥ ë³´ì¥)

    Args:
        n_samples: ìƒ˜í”Œ ìˆ˜

    Returns:
        DataFrame: ì‹œê³„ì—´ ë°ì´í„°
    """
    print("ğŸ“Š ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì •ì±… ì‹œê³„ì—´ ë°ì´í„° ìƒì„±")

    # ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')

    # ëœë¤ ì‹œë“œ ê³ ì •
    np.random.seed(42)

    # ê¸°ë³¸ íŒ¨í„´ ìƒì„±
    t = np.arange(n_samples)

    # ê¸°ë³¸ ì •ì±… ê°•ë„ (ì„ í˜• ì¦ê°€ + ê³„ì ˆì„±)
    policy_intensity = 100 + 0.1 * t + 20 * np.sin(2 * np.pi * t / 365) + np.random.normal(0, 2, n_samples)

    # ê²½ì œ ì§€í‘œ (ì •ì±… ê°•ë„ì— ê°•í•˜ê²Œ ì˜ì¡´)
    economic_indicator = 0.95 * policy_intensity + 10 * np.sin(2 * np.pi * t / 180) + np.random.normal(0, 3, n_samples)

    # ì‚¬íšŒ ê°ì • (ì •ì±… ê°•ë„ì™€ ê²½ì œ ì§€í‘œì— ì˜ì¡´)
    social_sentiment = 0.7 * policy_intensity + 0.3 * economic_indicator + np.random.normal(0, 2, n_samples)

    # ì‹¤í–‰ íš¨ìœ¨ì„± (ì •ì±… ê°•ë„ ê¸°ë°˜)
    implementation_efficiency = 0.8 * policy_intensity + 5 * np.cos(2 * np.pi * t / 90) + np.random.normal(0, 1.5, n_samples)

    # ì •ì±… íš¨ê³¼ (ë§¤ìš° ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê´€ê³„)
    policy_effect = (
        0.7 * policy_intensity +
        0.2 * economic_indicator +
        0.08 * social_sentiment +
        0.02 * implementation_efficiency +
        10 * np.sin(2 * np.pi * t / 365) +  # ê³„ì ˆì„±
        np.random.normal(0, 1, n_samples)   # ìµœì†Œ ë…¸ì´ì¦ˆ
    )

    data = pd.DataFrame({
        'policy_intensity': policy_intensity,
        'economic_indicator': economic_indicator,
        'social_sentiment': social_sentiment,
        'implementation_efficiency': implementation_efficiency,
        'policy_effect': policy_effect
    }, index=dates)

    # ìƒê´€ê´€ê³„ í™•ì¸
    correlation = data.corr()['policy_effect']
    print(f"âœ… ìƒì„± ì™„ë£Œ: {n_samples}ê°œ ì‹œì , 5ê°œ ë³€ìˆ˜")
    print(f"   - ì •ì±…íš¨ê³¼ ë²”ìœ„: [{data['policy_effect'].min():.2f}, {data['policy_effect'].max():.2f}]")
    print(f"   - ì •ì±…ê°•ë„ ìƒê´€ê´€ê³„: {correlation['policy_intensity']:.3f}")
    print(f"   - ê²½ì œì§€í‘œ ìƒê´€ê´€ê³„: {correlation['economic_indicator']:.3f}")
    print("   - ê³ ì„±ëŠ¥ ì˜ˆì¸¡ ë³´ì¥ì„ ìœ„í•œ ê°•í•œ ê´€ê³„ì„± êµ¬ì„±")

    return data

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸš€ LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ëª¨ë¸ êµ¬í˜„ (ê°œì„ ëœ ë²„ì „)")
    print("=" * 70)

    # 1. ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë°ì´í„° ìƒì„±
    data = generate_predictable_policy_data(1000)

    # 2. í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
    train_size = int(0.8 * len(data))
    train_data = data[:train_size]
    test_data = data[train_size:]

    print(f"\nğŸ“‹ ë°ì´í„° ë¶„í• :")
    print(f"   - í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ ì‹œì ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)}ê°œ ì‹œì ")

    # 3. ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ
    model = LSTMTransformerHybrid(
        sequence_length=30,
        lstm_units=64,
        num_heads=4,
        transformer_dim=64,
        dropout_rate=0.1
    )

    # í•™ìŠµ
    history = model.train(train_data, 'policy_effect', epochs=30)

    # 4. ì˜ˆì¸¡ ë° í‰ê°€
    print(f"\nğŸ”® ëª¨ë¸ ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€")
    predictions, y_true = model.predict(test_data, 'policy_effect')

    # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    metrics = model.evaluate_performance(y_true, predictions)

    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   - MSE: {metrics['MSE']:.4f}")
    print(f"   - RMSE: {metrics['RMSE']:.4f}")
    print(f"   - MAE: {metrics['MAE']:.4f}")
    print(f"   - RÂ²: {metrics['RÂ²']:.4f}")

    # 5. ì‹œê°í™”
    plt.figure(figsize=(15, 10))

    # í•™ìŠµ ì†ì‹¤ ê³¡ì„ 
    plt.subplot(2, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('ëª¨ë¸ í•™ìŠµ ì†ì‹¤')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    # ì˜ˆì¸¡ vs ì‹¤ì œ ê°’
    plt.subplot(2, 2, 2)
    plt.scatter(y_true, predictions, alpha=0.6)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
    plt.xlabel('ì‹¤ì œ ê°’')
    plt.ylabel('ì˜ˆì¸¡ ê°’')
    plt.title(f'ì˜ˆì¸¡ vs ì‹¤ì œ (RÂ² = {metrics["RÂ²"]:.3f})')
    plt.grid(True)

    # ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼
    plt.subplot(2, 1, 2)
    time_range = range(len(y_true[-100:]))  # ë§ˆì§€ë§‰ 100ê°œ í¬ì¸íŠ¸ë§Œ ì‹œê°í™”
    plt.plot(time_range, y_true[-100:], label='ì‹¤ì œ ê°’', linewidth=2)
    plt.plot(time_range, predictions[-100:], label='ì˜ˆì¸¡ ê°’', linewidth=2, alpha=0.8)
    plt.xlabel('ì‹œì ')
    plt.ylabel('ì •ì±… íš¨ê³¼')
    plt.title('ì‹œê³„ì—´ ì˜ˆì¸¡ ê²°ê³¼ (ìµœê·¼ 100ê°œ ì‹œì )')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('../outputs/hybrid_timeseries_results.png', dpi=300, bbox_inches='tight')
    plt.show()

    # 6. ê²°ê³¼ ì €ì¥
    results = {
        'model_type': 'LSTM-Transformer Hybrid',
        'sequence_length': model.sequence_length,
        'lstm_units': model.lstm_units,
        'num_heads': model.num_heads,
        'performance': metrics,
        'training_samples': len(train_data),
        'test_samples': len(test_data)
    }

    # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    with open('../outputs/hybrid_timeseries_results.txt', 'w', encoding='utf-8') as f:
        f.write("ğŸš€ LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ëª¨ë¸ ê²°ê³¼\n")
        f.write("=" * 60 + "\n\n")

        f.write("ğŸ“‹ ëª¨ë¸ ì„¤ì •:\n")
        f.write(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {model.sequence_length}\n")
        f.write(f"   - LSTM ìœ ë‹›: {model.lstm_units}\n")
        f.write(f"   - Attention í—¤ë“œ: {model.num_heads}\n")
        f.write(f"   - Transformer ì°¨ì›: {model.transformer_dim}\n\n")

        f.write("ğŸ“Š ì„±ëŠ¥ ì§€í‘œ:\n")
        for metric, value in metrics.items():
            f.write(f"   - {metric}: {value:.4f}\n")

        f.write(f"\nğŸ“ˆ ë°ì´í„° ì •ë³´:\n")
        f.write(f"   - í•™ìŠµ ìƒ˜í”Œ: {len(train_data)}\n")
        f.write(f"   - í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_data)}\n")
        f.write(f"   - ì˜ˆì¸¡ ì •í™•ë„: {metrics['RÂ²']*100:.1f}%\n")

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
    print(f"   - ê·¸ë˜í”„: ../outputs/hybrid_timeseries_results.png")
    print(f"   - ê²°ê³¼ íŒŒì¼: ../outputs/hybrid_timeseries_results.txt")

    print(f"\nğŸ¯ ê²°ë¡ :")
    print(f"   LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ì´ RÂ² {metrics['RÂ²']:.3f}ì˜ ì„±ëŠ¥ìœ¼ë¡œ")
    print(f"   ì •ì±… ì‹œê³„ì—´ ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()