#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì •ì±… ì§€ì› ì‹œìŠ¤í…œ
ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œì˜ ì •ì±… ë¶„ì„ ë° ì˜ì‚¬ê²°ì • ì§€ì›
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from typing import List, Dict, Any, Tuple
import json
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10

@dataclass
class PolicyRecommendation:
    """ì •ì±… ê¶Œì¥ì‚¬í•­ ë°ì´í„° í´ë˜ìŠ¤"""
    policy_name: str
    priority: float
    confidence: float
    impact_score: float
    rationale: str
    estimated_effect: Dict[str, float]

@dataclass
class AgentDecision:
    """ì—ì´ì „íŠ¸ ì˜ì‚¬ê²°ì • ë°ì´í„° í´ë˜ìŠ¤"""
    agent_type: str
    decision: str
    confidence: float
    reasoning: List[str]
    data_sources: List[str]

class PolicyAnalysisAgent:
    """ì •ì±… ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸"""

    def __init__(self, agent_id: str, specialization: str):
        """
        ì •ì±… ë¶„ì„ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”

        Parameters:
        agent_id (str): ì—ì´ì „íŠ¸ ì‹ë³„ì
        specialization (str): ì „ë¬¸ ë¶„ì•¼
        """
        self.agent_id = agent_id
        self.specialization = specialization
        self.model = None
        self.knowledge_base = {}
        self.decision_history = []

    def train_model(self, X: np.ndarray, y: np.ndarray, model_type='random_forest'):
        """
        ì—ì´ì „íŠ¸ ë‚´ë¶€ ëª¨ë¸ í•™ìŠµ

        Parameters:
        X (array): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜
        model_type (str): ëª¨ë¸ íƒ€ì…
        """
        if model_type == 'random_forest':
            self.model = RandomForestRegressor(
                n_estimators=100,
                max_depth=8,
                random_state=42,
                n_jobs=-1
            )
        elif model_type == 'gradient_boosting':
            self.model = GradientBoostingRegressor(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.1,
                random_state=42
            )

        self.model.fit(X, y)

        # ì„±ëŠ¥ í‰ê°€
        y_pred = self.model.predict(X)
        r2 = r2_score(y, y_pred)

        print(f"âœ… {self.agent_id} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ (RÂ²: {r2:.4f})")

    def analyze_policy_scenario(self, scenario_data: Dict[str, float]) -> PolicyRecommendation:
        """
        ì •ì±… ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„

        Parameters:
        scenario_data (dict): ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°

        Returns:
        PolicyRecommendation: ì •ì±… ê¶Œì¥ì‚¬í•­
        """
        # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ë³€í™˜
        features = np.array([list(scenario_data.values())]).reshape(1, -1)

        # ì˜ˆì¸¡ ìˆ˜í–‰
        if self.model is not None:
            prediction = self.model.predict(features)[0]

            # íŠ¹ì„± ì¤‘ìš”ë„ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
            if hasattr(self.model, 'feature_importances_'):
                importance_scores = self.model.feature_importances_
                confidence = np.mean(importance_scores) * 0.8 + 0.2
            else:
                confidence = 0.7
        else:
            prediction = np.random.uniform(50, 80)
            confidence = 0.5

        # ì •ì±… ê¶Œì¥ì‚¬í•­ ìƒì„±
        policy_name = f"{self.specialization}_ì •ì±…_ê°œì„ ì•ˆ"
        priority = min(prediction / 100, 1.0)
        impact_score = prediction

        # ì¶”ë¡  ë…¼ë¦¬ ìƒì„±
        rationale = self._generate_rationale(scenario_data, prediction)

        # ì˜ˆìƒ íš¨ê³¼ ê³„ì‚°
        estimated_effect = self._calculate_estimated_effect(scenario_data, prediction)

        recommendation = PolicyRecommendation(
            policy_name=policy_name,
            priority=priority,
            confidence=confidence,
            impact_score=impact_score,
            rationale=rationale,
            estimated_effect=estimated_effect
        )

        return recommendation

    def _generate_rationale(self, scenario_data: Dict[str, float], prediction: float) -> str:
        """ì¶”ë¡  ë…¼ë¦¬ ìƒì„±"""
        key_factors = []

        # ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì—ì„œ ì£¼ìš” ìš”ì¸ ì‹ë³„
        for key, value in scenario_data.items():
            if value > 0.6:  # ë†’ì€ ê°’
                key_factors.append(f"{key} ìˆ˜ì¤€ì´ ë†’ìŒ ({value:.2f})")
            elif value < 0.4:  # ë‚®ì€ ê°’
                key_factors.append(f"{key} ìˆ˜ì¤€ì´ ë‚®ìŒ ({value:.2f})")

        if prediction > 70:
            conclusion = "ê¸ì •ì ì¸ ì •ì±… íš¨ê³¼ê°€ ì˜ˆìƒë¨"
        elif prediction > 50:
            conclusion = "ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì •ì±… íš¨ê³¼ê°€ ì˜ˆìƒë¨"
        else:
            conclusion = "ì •ì±… íš¨ê³¼ ê°œì„ ì´ í•„ìš”í•¨"

        rationale = f"{self.specialization} ê´€ì ì—ì„œ {', '.join(key_factors[:2])}ë¥¼ ê³ ë ¤í•  ë•Œ, {conclusion}"

        return rationale

    def _calculate_estimated_effect(self, scenario_data: Dict[str, float], prediction: float) -> Dict[str, float]:
        """ì˜ˆìƒ íš¨ê³¼ ê³„ì‚°"""
        base_effect = prediction / 100

        estimated_effect = {
            'ê²½ì œì _íš¨ê³¼': base_effect * 0.8 + np.random.normal(0, 0.1),
            'ì‚¬íšŒì _íš¨ê³¼': base_effect * 0.9 + np.random.normal(0, 0.1),
            'í™˜ê²½ì _íš¨ê³¼': base_effect * 0.7 + np.random.normal(0, 0.1),
            'ì •ì¹˜ì _íš¨ê³¼': base_effect * 0.6 + np.random.normal(0, 0.1)
        }

        # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        estimated_effect = {k: np.clip(v, 0, 1) for k, v in estimated_effect.items()}

        return estimated_effect

    def make_decision(self, context: Dict[str, Any]) -> AgentDecision:
        """
        ì˜ì‚¬ê²°ì • ìˆ˜í–‰

        Parameters:
        context (dict): ì˜ì‚¬ê²°ì • ì»¨í…ìŠ¤íŠ¸

        Returns:
        AgentDecision: ì—ì´ì „íŠ¸ ì˜ì‚¬ê²°ì •
        """
        # ê²°ì • ì˜µì…˜ë“¤
        options = ['ì •ì±…_ìŠ¹ì¸', 'ì •ì±…_ìˆ˜ì •', 'ì •ì±…_ê±°ë¶€', 'ì¶”ê°€_ë¶„ì„_í•„ìš”']

        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for option in options:
            score = np.random.uniform(0, 1)

            # ì „ë¬¸ë¶„ì•¼ë³„ ê°€ì¤‘ì¹˜ ì ìš©
            if self.specialization == 'ê²½ì œ' and 'economic' in option.lower():
                score *= 1.2
            elif self.specialization == 'ì‚¬íšŒ' and 'social' in option.lower():
                score *= 1.2
            elif self.specialization == 'í™˜ê²½' and 'environmental' in option.lower():
                score *= 1.2

            scores[option] = score

        # ìµœê³  ì ìˆ˜ ì˜µì…˜ ì„ íƒ
        best_option = max(scores.keys(), key=lambda x: scores[x])
        confidence = scores[best_option]

        # ì¶”ë¡  ê³¼ì • ìƒì„±
        reasoning = [
            f"{self.specialization} ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„",
            f"ì»¨í…ìŠ¤íŠ¸ ìš”ì†Œ {len(context)}ê°œ ê³ ë ¤",
            f"ìµœì  ì„ íƒ: {best_option} (ì‹ ë¢°ë„: {confidence:.3f})"
        ]

        # ë°ì´í„° ì†ŒìŠ¤
        data_sources = ['ë‚´ë¶€_ëª¨ë¸', 'ì „ë¬¸_ì§€ì‹', 'ê³¼ê±°_ì‚¬ë¡€']

        decision = AgentDecision(
            agent_type=self.specialization,
            decision=best_option,
            confidence=confidence,
            reasoning=reasoning,
            data_sources=data_sources
        )

        self.decision_history.append(decision)

        return decision

class MultiAgentPolicySystem:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì •ì±… ì§€ì› ì‹œìŠ¤í…œ"""

    def __init__(self):
        """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.agents = {}
        self.coordination_history = []
        self.consensus_threshold = 0.7

    def create_agents(self, specializations: List[str]):
        """
        ì „ë¬¸ ë¶„ì•¼ë³„ ì—ì´ì „íŠ¸ ìƒì„±

        Parameters:
        specializations (list): ì „ë¬¸ ë¶„ì•¼ ëª©ë¡
        """
        for spec in specializations:
            agent_id = f"agent_{spec}"
            self.agents[agent_id] = PolicyAnalysisAgent(agent_id, spec)

        print(f"âœ… {len(specializations)}ê°œ ì „ë¬¸ ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ")
        for spec in specializations:
            print(f"   - {spec} ì „ë¬¸ ì—ì´ì „íŠ¸")

    def train_all_agents(self, X: np.ndarray, y: np.ndarray):
        """
        ëª¨ë“  ì—ì´ì „íŠ¸ í•™ìŠµ

        Parameters:
        X (array): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜
        """
        print("\nğŸ¤– ëª¨ë“  ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘")

        for agent_id, agent in self.agents.items():
            # ê° ì—ì´ì „íŠ¸ë§ˆë‹¤ ì•½ê°„ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
            model_types = ['random_forest', 'gradient_boosting']
            model_type = model_types[len(agent_id) % 2]

            agent.train_model(X, y, model_type)

        print("âœ… ëª¨ë“  ì—ì´ì „íŠ¸ í•™ìŠµ ì™„ë£Œ")

    def generate_policy_data(self, n_samples=1000):
        """
        ì •ì±… ë¶„ì„ìš© ë°ì´í„° ìƒì„±
        â€» ë³¸ ë°ì´í„°ëŠ” êµìœ¡ ëª©ì ì˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤

        Parameters:
        n_samples (int): ìƒ˜í”Œ ìˆ˜

        Returns:
        tuple: (X, y, feature_names) íŠ¹ì„±, íƒ€ê²Ÿ, íŠ¹ì„±ëª…
        """
        np.random.seed(42)

        feature_names = [
            'ê²½ì œì„±ì¥ë¥ ', 'êµìœ¡íˆ¬ììœ¨', 'ì¸í”„ë¼ìˆ˜ì¤€',
            'ì‚¬íšŒë³µì§€ìˆ˜ì¤€', 'í™˜ê²½í’ˆì§ˆì§€ìˆ˜', 'ì •ì¹˜ì•ˆì •ì„±'
        ]

        n_features = len(feature_names)

        # ê¸°ë³¸ íŠ¹ì„± ìƒì„± (0-1 ì •ê·œí™”ëœ ê°’)
        X = np.random.uniform(0, 1, (n_samples, n_features))

        # ë³µì¡í•œ ì •ì±… íš¨ê³¼ í•¨ìˆ˜
        policy_effectiveness = (
            0.3 * X[:, 0] +                     # ê²½ì œì„±ì¥ë¥ 
            0.25 * X[:, 1] +                    # êµìœ¡íˆ¬ììœ¨
            0.2 * X[:, 2] +                     # ì¸í”„ë¼ìˆ˜ì¤€
            0.15 * X[:, 3] +                    # ì‚¬íšŒë³µì§€ìˆ˜ì¤€
            0.1 * X[:, 4] +                     # í™˜ê²½í’ˆì§ˆì§€ìˆ˜
            0.05 * X[:, 5] +                    # ì •ì¹˜ì•ˆì •ì„±
            0.1 * X[:, 0] * X[:, 1] +           # ê²½ì œ-êµìœ¡ ìƒí˜¸ì‘ìš©
            0.05 * X[:, 2] * X[:, 3] +          # ì¸í”„ë¼-ë³µì§€ ìƒí˜¸ì‘ìš©
            0.1 * np.random.randn(n_samples)    # ë…¸ì´ì¦ˆ
        )

        # 0-100 ì ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§
        y = 50 + 30 * policy_effectiveness
        y = np.clip(y, 0, 100)

        # DataFrame ìƒì„±
        X_df = pd.DataFrame(X, columns=feature_names)

        print(f"âœ… ì •ì±… ë¶„ì„ìš© ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print(f"   - ìƒ˜í”Œ ìˆ˜: {n_samples}")
        print(f"   - íŠ¹ì„± ìˆ˜: {n_features}")
        print(f"   - ì •ì±…íš¨ê³¼ ë²”ìœ„: [{y.min():.2f}, {y.max():.2f}]")

        return X_df, y, feature_names

    def coordinate_agents(self, scenario: Dict[str, float]) -> List[PolicyRecommendation]:
        """
        ì—ì´ì „íŠ¸ ê°„ í˜‘ë ¥ ë° ì¡°ì •

        Parameters:
        scenario (dict): ë¶„ì„í•  ì‹œë‚˜ë¦¬ì˜¤

        Returns:
        list: ê° ì—ì´ì „íŠ¸ì˜ ì •ì±… ê¶Œì¥ì‚¬í•­
        """
        print(f"\nğŸ¤ ì—ì´ì „íŠ¸ í˜‘ë ¥ ë¶„ì„ ì‹œì‘")
        print(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario}")

        recommendations = []

        for agent_id, agent in self.agents.items():
            print(f"   ğŸ”„ {agent.specialization} ì—ì´ì „íŠ¸ ë¶„ì„ ì¤‘...")

            recommendation = agent.analyze_policy_scenario(scenario)
            recommendations.append(recommendation)

            print(f"      âœ… ì™„ë£Œ - ìš°ì„ ìˆœìœ„: {recommendation.priority:.3f}, "
                  f"ì‹ ë¢°ë„: {recommendation.confidence:.3f}")

        print("âœ… ëª¨ë“  ì—ì´ì „íŠ¸ ë¶„ì„ ì™„ë£Œ")

        return recommendations

    def make_collective_decision(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ìˆ˜í–‰

        Parameters:
        context (dict): ì˜ì‚¬ê²°ì • ì»¨í…ìŠ¤íŠ¸

        Returns:
        dict: ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ê²°ê³¼
        """
        print(f"\nğŸ—³ï¸ ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ì‹œì‘")

        agent_decisions = []
        decision_scores = {}

        # ê° ì—ì´ì „íŠ¸ì˜ ì˜ì‚¬ê²°ì • ìˆ˜ì§‘
        for agent_id, agent in self.agents.items():
            decision = agent.make_decision(context)
            agent_decisions.append(decision)

            # ì˜ì‚¬ê²°ì •ë³„ ì ìˆ˜ ì§‘ê³„
            if decision.decision not in decision_scores:
                decision_scores[decision.decision] = []
            decision_scores[decision.decision].append(decision.confidence)

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_scores = {}
        for decision, scores in decision_scores.items():
            weighted_scores[decision] = np.mean(scores)

        # ìµœì¢… ì˜ì‚¬ê²°ì •
        final_decision = max(weighted_scores.keys(), key=lambda x: weighted_scores[x])
        consensus_score = weighted_scores[final_decision]

        # í•©ì˜ ì—¬ë¶€ íŒë‹¨
        consensus_reached = consensus_score >= self.consensus_threshold

        result = {
            'final_decision': final_decision,
            'consensus_score': consensus_score,
            'consensus_reached': consensus_reached,
            'agent_decisions': agent_decisions,
            'vote_distribution': decision_scores
        }

        print(f"âœ… ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ì™„ë£Œ")
        print(f"   - ìµœì¢… ê²°ì •: {final_decision}")
        print(f"   - í•©ì˜ ì ìˆ˜: {consensus_score:.3f}")
        print(f"   - í•©ì˜ ë‹¬ì„±: {'ì˜ˆ' if consensus_reached else 'ì•„ë‹ˆì˜¤'}")

        self.coordination_history.append(result)

        return result

    def generate_comprehensive_report(self, recommendations: List[PolicyRecommendation],
                                    collective_decision: Dict[str, Any]) -> str:
        """
        ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±

        Parameters:
        recommendations (list): ì •ì±… ê¶Œì¥ì‚¬í•­ë“¤
        collective_decision (dict): ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ê²°ê³¼

        Returns:
        str: ì¢…í•© ë³´ê³ ì„œ
        """
        report = []
        report.append("="*80)
        report.append("AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì •ì±… ë¶„ì„ ì¢…í•© ë³´ê³ ì„œ")
        report.append("="*80)

        # ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
        report.append("\n1. ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼")
        report.append("-" * 50)

        for i, rec in enumerate(recommendations, 1):
            report.append(f"\n[ì—ì´ì „íŠ¸ {i}: {rec.policy_name}]")
            report.append(f"  ìš°ì„ ìˆœìœ„: {rec.priority:.3f}")
            report.append(f"  ì‹ ë¢°ë„: {rec.confidence:.3f}")
            report.append(f"  ì˜í–¥ë„: {rec.impact_score:.2f}")
            report.append(f"  ì¶”ë¡ : {rec.rationale}")

            report.append("  ì˜ˆìƒ íš¨ê³¼:")
            for effect, value in rec.estimated_effect.items():
                report.append(f"    - {effect}: {value:.3f}")

        # ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ê²°ê³¼
        report.append("\n\n2. ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ê²°ê³¼")
        report.append("-" * 50)
        report.append(f"ìµœì¢… ê²°ì •: {collective_decision['final_decision']}")
        report.append(f"í•©ì˜ ì ìˆ˜: {collective_decision['consensus_score']:.3f}")
        report.append(f"í•©ì˜ ë‹¬ì„±: {'ì˜ˆ' if collective_decision['consensus_reached'] else 'ì•„ë‹ˆì˜¤'}")

        # íˆ¬í‘œ ë¶„í¬
        report.append("\níˆ¬í‘œ ë¶„í¬:")
        for decision, scores in collective_decision['vote_distribution'].items():
            avg_score = np.mean(scores)
            vote_count = len(scores)
            report.append(f"  - {decision}: {vote_count}í‘œ (í‰ê·  ì‹ ë¢°ë„: {avg_score:.3f})")

        # ì¢…í•© ê¶Œì¥ì‚¬í•­
        report.append("\n\n3. ì¢…í•© ê¶Œì¥ì‚¬í•­")
        report.append("-" * 50)

        # ìš°ì„ ìˆœìœ„ ë†’ì€ ê¶Œì¥ì‚¬í•­
        top_recommendation = max(recommendations, key=lambda x: x.priority)
        report.append(f"ìµœìš°ì„  ì •ì±…: {top_recommendation.policy_name}")
        report.append(f"ê¶Œì¥ ì´ìœ : {top_recommendation.rationale}")

        # ì‹ ë¢°ë„ ë†’ì€ ê¶Œì¥ì‚¬í•­
        most_confident = max(recommendations, key=lambda x: x.confidence)
        report.append(f"\nê°€ì¥ í™•ì‹¤í•œ ì •ì±…: {most_confident.policy_name}")
        report.append(f"ì‹ ë¢°ë„: {most_confident.confidence:.3f}")

        return "\n".join(report)

    def visualize_agent_analysis(self, recommendations: List[PolicyRecommendation],
                               save_path='practice/chapter05/outputs/agent_analysis.png'):
        """
        ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”

        Parameters:
        recommendations (list): ì •ì±… ê¶Œì¥ì‚¬í•­ë“¤
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. ìš°ì„ ìˆœìœ„ ë¹„êµ
        ax1 = axes[0, 0]
        agent_names = [rec.policy_name.split('_')[0] for rec in recommendations]
        priorities = [rec.priority for rec in recommendations]

        bars1 = ax1.bar(agent_names, priorities, alpha=0.7, color='skyblue')
        ax1.set_title('ì—ì´ì „íŠ¸ë³„ ì •ì±… ìš°ì„ ìˆœìœ„')
        ax1.set_ylabel('ìš°ì„ ìˆœìœ„ ì ìˆ˜')
        ax1.tick_params(axis='x', rotation=45)

        # ê°’ í‘œì‹œ
        for bar, priority in zip(bars1, priorities):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{priority:.3f}', ha='center', va='bottom')

        # 2. ì‹ ë¢°ë„ ë¹„êµ
        ax2 = axes[0, 1]
        confidences = [rec.confidence for rec in recommendations]

        bars2 = ax2.bar(agent_names, confidences, alpha=0.7, color='lightgreen')
        ax2.set_title('ì—ì´ì „íŠ¸ë³„ ì‹ ë¢°ë„')
        ax2.set_ylabel('ì‹ ë¢°ë„')
        ax2.tick_params(axis='x', rotation=45)

        for bar, confidence in zip(bars2, confidences):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{confidence:.3f}', ha='center', va='bottom')

        # 3. ì˜í–¥ë„ ì ìˆ˜ ë¹„êµ
        ax3 = axes[1, 0]
        impact_scores = [rec.impact_score for rec in recommendations]

        # ë°” ì°¨íŠ¸ë¡œ ê° ì—ì´ì „íŠ¸ë³„ ì˜í–¥ë„ í‘œì‹œ
        bars3 = ax3.bar(agent_names, impact_scores, alpha=0.7, color='orange')
        ax3.set_title('Agent Impact Scores')
        ax3.set_xlabel('Agent')
        ax3.set_ylabel('Impact Score')
        ax3.tick_params(axis='x', rotation=45)

        for bar, score in zip(bars3, impact_scores):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                    f'{score:.1f}', ha='center', va='bottom')

        # 4. ì˜ˆìƒ íš¨ê³¼ ë¹„êµ (ì²« ë²ˆì§¸ ê¶Œì¥ì‚¬í•­ ê¸°ì¤€)
        ax4 = axes[1, 1]

        if recommendations:
            effects = recommendations[0].estimated_effect
            effect_names = list(effects.keys())
            effect_values = list(effects.values())

            bars4 = ax4.bar(effect_names, effect_values, alpha=0.7, color='purple')
            ax4.set_title('ì˜ˆìƒ íš¨ê³¼ ë¶„ì„ (ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸)')
            ax4.set_ylabel('íš¨ê³¼ ì ìˆ˜')
            ax4.tick_params(axis='x', rotation=45)

            for bar, value in zip(bars4, effect_values):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{value:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ ì—ì´ì „íŠ¸ ë¶„ì„ ì‹œê°í™” ì €ì¥: {save_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì •ì±… ì§€ì› ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*70)

    # 1. ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ìƒì„±
    print("ğŸ¤– ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    mas = MultiAgentPolicySystem()

    # 2. ì „ë¬¸ ë¶„ì•¼ë³„ ì—ì´ì „íŠ¸ ìƒì„±
    specializations = ['ê²½ì œ', 'ì‚¬íšŒ', 'í™˜ê²½', 'ê¸°ìˆ ', 'ì •ì¹˜']
    mas.create_agents(specializations)

    # 3. í•™ìŠµ ë°ì´í„° ìƒì„±
    print("\nğŸ“Š ì •ì±… ë¶„ì„ìš© ë°ì´í„° ìƒì„±")
    X, y, feature_names = mas.generate_policy_data(n_samples=1200)

    # 4. ëª¨ë“  ì—ì´ì „íŠ¸ í•™ìŠµ
    mas.train_all_agents(X.values, y)

    # 5. ì •ì±… ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
    print("\nğŸ“‹ ì •ì±… ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„")
    test_scenario = {
        'ê²½ì œì„±ì¥ë¥ ': 0.7,
        'êµìœ¡íˆ¬ììœ¨': 0.8,
        'ì¸í”„ë¼ìˆ˜ì¤€': 0.6,
        'ì‚¬íšŒë³µì§€ìˆ˜ì¤€': 0.5,
        'í™˜ê²½í’ˆì§ˆì§€ìˆ˜': 0.4,
        'ì •ì¹˜ì•ˆì •ì„±': 0.9
    }

    # ì—ì´ì „íŠ¸ í˜‘ë ¥ ë¶„ì„
    recommendations = mas.coordinate_agents(test_scenario)

    # 6. ì§‘ë‹¨ ì˜ì‚¬ê²°ì •
    print("\nğŸ—³ï¸ ì§‘ë‹¨ ì˜ì‚¬ê²°ì • ìˆ˜í–‰")
    decision_context = {
        'urgency': 'high',
        'budget_constraint': 'medium',
        'political_feasibility': 'high',
        'public_support': 'medium'
    }

    collective_decision = mas.make_collective_decision(decision_context)

    # 7. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
    print("\nğŸ“„ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±")
    report = mas.generate_comprehensive_report(recommendations, collective_decision)

    # ë³´ê³ ì„œ ì €ì¥
    with open('practice/chapter05/outputs/ai_agent_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)

    print("ğŸ“„ ë³´ê³ ì„œ ì €ì¥: practice/chapter05/outputs/ai_agent_report.txt")

    # 8. ì‹œê°í™”
    print("\nğŸ“ˆ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”")
    mas.visualize_agent_analysis(recommendations)

    # 9. ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ì¶”ê°€ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    additional_scenarios = [
        {'ê²½ì œì„±ì¥ë¥ ': 0.3, 'êµìœ¡íˆ¬ììœ¨': 0.4, 'ì¸í”„ë¼ìˆ˜ì¤€': 0.3,
         'ì‚¬íšŒë³µì§€ìˆ˜ì¤€': 0.2, 'í™˜ê²½í’ˆì§ˆì§€ìˆ˜': 0.8, 'ì •ì¹˜ì•ˆì •ì„±': 0.6},
        {'ê²½ì œì„±ì¥ë¥ ': 0.9, 'êµìœ¡íˆ¬ììœ¨': 0.9, 'ì¸í”„ë¼ìˆ˜ì¤€': 0.8,
         'ì‚¬íšŒë³µì§€ìˆ˜ì¤€': 0.7, 'í™˜ê²½í’ˆì§ˆì§€ìˆ˜': 0.6, 'ì •ì¹˜ì•ˆì •ì„±': 0.8}
    ]

    for i, scenario in enumerate(additional_scenarios, 1):
        print(f"\n   ğŸ“Œ ì‹œë‚˜ë¦¬ì˜¤ {i} ë¶„ì„:")
        scenario_recs = mas.coordinate_agents(scenario)

        # ìµœê³  ìš°ì„ ìˆœìœ„ ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        top_rec = max(scenario_recs, key=lambda x: x.priority)
        print(f"      ìµœìš°ì„  ì •ì±…: {top_rec.policy_name}")
        print(f"      ìš°ì„ ìˆœìœ„: {top_rec.priority:.3f}")
        print(f"      ì¶”ë¡ : {top_rec.rationale}")

    # 10. ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ¯ AI ì—ì´ì „íŠ¸ ê¸°ë°˜ ì •ì±… ì§€ì› ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("="*70)

    # ì„±ëŠ¥ ìš”ì•½
    avg_confidence = np.mean([rec.confidence for rec in recommendations])
    avg_priority = np.mean([rec.priority for rec in recommendations])

    print(f"ğŸ“Š ì‹œìŠ¤í…œ ì„±ëŠ¥:")
    print(f"   - í™œì„± ì—ì´ì „íŠ¸: {len(mas.agents)}ê°œ")
    print(f"   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.3f}")
    print(f"   - í‰ê·  ìš°ì„ ìˆœìœ„: {avg_priority:.3f}")
    print(f"   - í•©ì˜ ë‹¬ì„±ë¥ : {collective_decision['consensus_score']:.1%}")

    print(f"\nğŸ” ì—ì´ì „íŠ¸ í˜‘ë ¥ ë¶„ì„:")
    print(f"   - ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„: 3ê°œ ì™„ë£Œ")
    print(f"   - ì •ì±… ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ ìƒì„±")
    print(f"   - ì§‘ë‹¨ ì˜ì‚¬ê²°ì •: {'ì„±ê³µ' if collective_decision['consensus_reached'] else 'ë¶€ë¶„ ì„±ê³µ'}")

    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
    print("   - practice/chapter05/outputs/ai_agent_report.txt")
    print("   - practice/chapter05/outputs/agent_analysis.png")

    print("\nâœ… ëª¨ë“  AI ì—ì´ì „íŠ¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()