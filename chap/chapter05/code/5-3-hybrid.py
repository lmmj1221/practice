#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ëª¨ë¸
ì •ì±… ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬í˜„
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, LSTM, Dense, Dropout, LayerNormalization,
    MultiHeadAttention, GlobalAveragePooling1D, Add,
    Bidirectional, Concatenate
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10

class LSTMTransformerHybrid:
    """LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self, sequence_length=20, num_features=5, lstm_units=64, attention_heads=8, d_model=128):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì´ˆê¸°í™”

        Parameters:
        sequence_length (int): ì‹œí€€ìŠ¤ ê¸¸ì´
        num_features (int): íŠ¹ì„± ìˆ˜
        lstm_units (int): LSTM ìœ ë‹› ìˆ˜
        attention_heads (int): ì–´í…ì…˜ í—¤ë“œ ìˆ˜
        d_model (int): ëª¨ë¸ ì°¨ì›
        """
        self.sequence_length = sequence_length
        self.num_features = num_features
        self.lstm_units = lstm_units
        self.attention_heads = attention_heads
        self.d_model = d_model
        self.model = None
        self.scaler = MinMaxScaler()
        self.history = None

    def generate_policy_timeseries(self, n_samples=2000, noise_level=0.1):
        """
        ì •ì±… ê´€ë ¨ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
        â€» ë³¸ ë°ì´í„°ëŠ” êµìœ¡ ëª©ì ì˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤

        Parameters:
        n_samples (int): ìƒì„±í•  ì‹œê³„ì—´ ê¸¸ì´
        noise_level (float): ë…¸ì´ì¦ˆ ë ˆë²¨

        Returns:
        DataFrame: ìƒì„±ëœ ì‹œê³„ì—´ ë°ì´í„°
        """
        np.random.seed(42)

        # ì‹œê°„ ì¸ë±ìŠ¤ ìƒì„±
        time_idx = np.arange(n_samples)

        # ê¸°ë³¸ íŠ¸ë Œë“œì™€ ê³„ì ˆì„± íŒ¨í„´
        trend = 0.001 * time_idx + 2.0
        seasonal = 0.5 * np.sin(2 * np.pi * time_idx / 12) + 0.3 * np.sin(2 * np.pi * time_idx / 52)

        # ì •ì±… ê´€ë ¨ íŠ¹ì„±ë“¤ ìƒì„±
        features = {}

        # 1. ê²½ì œì„±ì¥ë¥  (íŠ¸ë Œë“œ + ê³„ì ˆì„± + ìˆœí™˜ íŒ¨í„´)
        features['ê²½ì œì„±ì¥ë¥ '] = (trend +
                               seasonal +
                               0.2 * np.sin(2 * np.pi * time_idx / 20) +
                               noise_level * np.random.randn(n_samples))

        # 2. ì‹¤ì—…ë¥  (ì—­ìˆœí™˜ íŒ¨í„´)
        features['ì‹¤ì—…ë¥ '] = (5.0 - 0.5 * features['ê²½ì œì„±ì¥ë¥ '] +
                            0.3 * np.sin(2 * np.pi * time_idx / 30 + np.pi) +
                            noise_level * np.random.randn(n_samples))

        # 3. ì¸í”Œë ˆì´ì…˜ìœ¨ (ì§€ì—°ëœ ê²½ì œì„±ì¥ë¥  ë°˜ì‘)
        inflation_base = np.roll(features['ê²½ì œì„±ì¥ë¥ '], 3) * 0.4 + 2.0
        features['ì¸í”Œë ˆì´ì…˜ìœ¨'] = (inflation_base +
                               0.2 * np.sin(2 * np.pi * time_idx / 8) +
                               noise_level * np.random.randn(n_samples))

        # 4. ì •ë¶€ì§€ì¶œ (ì •ì±… ê°œì… ì‹œë®¬ë ˆì´ì…˜)
        gov_spending = 20.0 + 0.01 * time_idx
        # ì •ì±… ì¶©ê²© ì¶”ê°€ (íŠ¹ì • ì‹œì ì—ì„œ í° ë³€í™”)
        shock_points = [500, 1000, 1500]
        for shock in shock_points:
            if shock < n_samples:
                gov_spending[shock:shock+50] += 5.0 * np.exp(-np.arange(50) / 10)

        features['ì •ë¶€ì§€ì¶œë¹„ìœ¨'] = gov_spending + noise_level * np.random.randn(n_samples)

        # 5. ì •ì±…íš¨ê³¼ì§€ìˆ˜ (ë³µí•© ì§€í‘œ)
        features['ì •ì±…íš¨ê³¼ì§€ìˆ˜'] = (0.4 * features['ê²½ì œì„±ì¥ë¥ '] -
                               0.2 * features['ì‹¤ì—…ë¥ '] -
                               0.1 * features['ì¸í”Œë ˆì´ì…˜ìœ¨'] +
                               0.02 * features['ì •ë¶€ì§€ì¶œë¹„ìœ¨'] +
                               noise_level * np.random.randn(n_samples))

        # DataFrame ìƒì„±
        df = pd.DataFrame(features)
        df.index = pd.date_range(start='2020-01-01', periods=n_samples, freq='W')

        print(f"âœ… ì •ì±… ì‹œê³„ì—´ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print(f"   - ì‹œê³„ì—´ ê¸¸ì´: {n_samples}")
        print(f"   - íŠ¹ì„± ìˆ˜: {len(features)}")
        print(f"   - ê¸°ê°„: {df.index[0]} ~ {df.index[-1]}")

        return df

    def create_sequences(self, data, target_col='ì •ì±…íš¨ê³¼ì§€ìˆ˜'):
        """
        ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜

        Parameters:
        data (DataFrame): ì‹œê³„ì—´ ë°ì´í„°
        target_col (str): íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

        Returns:
        tuple: (X, y) ì‹œí€€ìŠ¤ ë°ì´í„°
        """
        # ë°ì´í„° ì •ê·œí™”
        scaled_data = self.scaler.fit_transform(data)
        scaled_df = pd.DataFrame(scaled_data, columns=data.columns, index=data.index)

        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì¸ë±ìŠ¤
        target_idx = data.columns.get_loc(target_col)

        X, y = [], []

        for i in range(self.sequence_length, len(scaled_df)):
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ëª¨ë“  íŠ¹ì„±)
            X.append(scaled_data[i-self.sequence_length:i])
            # íƒ€ê²Ÿ (ë‹¤ìŒ ì‹œì ì˜ íƒ€ê²Ÿ ê°’)
            y.append(scaled_data[i, target_idx])

        X = np.array(X)
        y = np.array(y)

        print(f"âœ… ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ")
        print(f"   - ì…ë ¥ í˜•íƒœ: {X.shape}")
        print(f"   - íƒ€ê²Ÿ í˜•íƒœ: {y.shape}")

        return X, y

    def build_hybrid_model(self):
        """
        LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•

        Returns:
        Model: êµ¬ì¶•ëœ ëª¨ë¸
        """
        # ì…ë ¥ ì¸µ
        inputs = Input(shape=(self.sequence_length, self.num_features), name='input_sequence')

        # LSTM ë¸Œëœì¹˜
        lstm_out = Bidirectional(
            LSTM(self.lstm_units, return_sequences=True, dropout=0.1, recurrent_dropout=0.1),
            name='bidirectional_lstm'
        )(inputs)

        lstm_out = LayerNormalization(name='lstm_norm')(lstm_out)

        # Transformer ë¸Œëœì¹˜
        # Multi-Head Attention
        attention_out = MultiHeadAttention(
            num_heads=self.attention_heads,
            key_dim=self.d_model // self.attention_heads,
            name='multi_head_attention'
        )(inputs, inputs)

        # Add & Norm
        attention_out = Add(name='attention_add')([inputs, attention_out])
        attention_out = LayerNormalization(name='attention_norm')(attention_out)

        # Feed Forward Network
        ffn_out = Dense(self.d_model * 2, activation='relu', name='ffn_1')(attention_out)
        ffn_out = Dropout(0.1, name='ffn_dropout_1')(ffn_out)
        ffn_out = Dense(self.num_features, name='ffn_2')(ffn_out)
        ffn_out = Dropout(0.1, name='ffn_dropout_2')(ffn_out)

        # Add & Norm
        transformer_out = Add(name='ffn_add')([attention_out, ffn_out])
        transformer_out = LayerNormalization(name='transformer_norm')(transformer_out)

        # ë¸Œëœì¹˜ ê²°í•©
        # LSTM ì¶œë ¥ì„ Transformer ì°¨ì›ì— ë§ê²Œ ì¡°ì •
        lstm_projected = Dense(self.num_features, name='lstm_projection')(lstm_out)

        # ë‘ ë¸Œëœì¹˜ ê²°í•©
        combined = Add(name='branch_combination')([lstm_projected, transformer_out])

        # Global Average Pooling
        pooled = GlobalAveragePooling1D(name='global_avg_pool')(combined)

        # Dense ì¸µë“¤
        dense_out = Dense(128, activation='relu', name='dense_1')(pooled)
        dense_out = Dropout(0.3, name='dense_dropout_1')(dense_out)

        dense_out = Dense(64, activation='relu', name='dense_2')(dense_out)
        dense_out = Dropout(0.3, name='dense_dropout_2')(dense_out)

        dense_out = Dense(32, activation='relu', name='dense_3')(dense_out)
        dense_out = Dropout(0.2, name='dense_dropout_3')(dense_out)

        # ì¶œë ¥ ì¸µ
        outputs = Dense(1, name='output')(dense_out)

        # ëª¨ë¸ ìƒì„±
        self.model = Model(inputs=inputs, outputs=outputs, name='LSTM_Transformer_Hybrid')

        # ì»´íŒŒì¼
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )

        print("âœ… LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {self.model.count_params():,}")

        return self.model

    def train_model(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):
        """
        ëª¨ë¸ í•™ìŠµ

        Parameters:
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        epochs (int): ì—í¬í¬ ìˆ˜
        batch_size (int): ë°°ì¹˜ í¬ê¸°

        Returns:
        History: í•™ìŠµ ì´ë ¥
        """
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-6,
                verbose=1
            )
        ]

        print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

        # ëª¨ë¸ í•™ìŠµ
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )

        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

        return self.history

    def evaluate_model(self, X_test, y_test):
        """
        ëª¨ë¸ í‰ê°€

        Parameters:
        X_test, y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°

        Returns:
        dict: í‰ê°€ ê²°ê³¼
        """
        # ì˜ˆì¸¡
        y_pred = self.model.predict(X_test, verbose=0)

        # í‰ê°€ ì§€í‘œ ê³„ì‚°
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(y_test, y_pred)

        results = {
            'MSE': mse,
            'MAE': mae,
            'RMSE': rmse,
            'RÂ²': r2,
            'predictions': y_pred.flatten()
        }

        print(f"ğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
        print(f"   - MSE: {mse:.6f}")
        print(f"   - MAE: {mae:.6f}")
        print(f"   - RMSE: {rmse:.6f}")
        print(f"   - RÂ²: {r2:.6f}")

        return results

    def plot_training_history(self, save_path='practice/chapter05/outputs/training_history.png'):
        """
        í•™ìŠµ ì´ë ¥ ì‹œê°í™”

        Parameters:
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        if self.history is None:
            print("âš ï¸ í•™ìŠµ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # Loss ê·¸ë˜í”„
        ax1.plot(self.history.history['loss'], label='í•™ìŠµ Loss', linewidth=2)
        ax1.plot(self.history.history['val_loss'], label='ê²€ì¦ Loss', linewidth=2)
        ax1.set_title('ëª¨ë¸ Loss ë³€í™”')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # MAE ê·¸ë˜í”„
        ax2.plot(self.history.history['mae'], label='í•™ìŠµ MAE', linewidth=2)
        ax2.plot(self.history.history['val_mae'], label='ê²€ì¦ MAE', linewidth=2)
        ax2.set_title('ëª¨ë¸ MAE ë³€í™”')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('MAE')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ í•™ìŠµ ì´ë ¥ ì €ì¥: {save_path}")

    def plot_predictions(self, y_true, y_pred, save_path='practice/chapter05/outputs/hybrid_predictions.png'):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”

        Parameters:
        y_true: ì‹¤ì œ ê°’
        y_pred: ì˜ˆì¸¡ ê°’
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        # ì‹œê³„ì—´ ë¹„êµ
        time_steps = range(len(y_true))
        ax1.plot(time_steps, y_true, label='ì‹¤ì œ ê°’', alpha=0.8, linewidth=1.5)
        ax1.plot(time_steps, y_pred, label='ì˜ˆì¸¡ ê°’', alpha=0.8, linewidth=1.5)
        ax1.set_title('ì‹œê³„ì—´ ì˜ˆì¸¡ ë¹„êµ')
        ax1.set_xlabel('ì‹œê°„ ë‹¨ê³„')
        ax1.set_ylabel('ì •ê·œí™”ëœ ê°’')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # ì‚°ì ë„
        ax2.scatter(y_true, y_pred, alpha=0.6, s=20)
        ax2.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)
        ax2.set_xlabel('ì‹¤ì œ ê°’')
        ax2.set_ylabel('ì˜ˆì¸¡ ê°’')
        ax2.set_title('ì˜ˆì¸¡ ì •í™•ë„ ì‚°ì ë„')
        ax2.grid(True, alpha=0.3)

        # RÂ² ê°’ í‘œì‹œ
        r2 = r2_score(y_true, y_pred)
        ax2.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax2.transAxes,
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥: {save_path}")

    def get_attention_weights(self, X_sample):
        """
        ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ì¶”ì¶œ

        Parameters:
        X_sample: ì…ë ¥ ìƒ˜í”Œ

        Returns:
        array: ì–´í…ì…˜ ê°€ì¤‘ì¹˜
        """
        # ì–´í…ì…˜ ì¸µì˜ ì¶œë ¥ì„ ì–»ê¸° ìœ„í•œ ì¤‘ê°„ ëª¨ë¸ ìƒì„±
        attention_layer = None
        for layer in self.model.layers:
            if isinstance(layer, MultiHeadAttention):
                attention_layer = layer
                break

        if attention_layer is None:
            print("âš ï¸ ì–´í…ì…˜ ì¸µì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # ì¤‘ê°„ ëª¨ë¸ ìƒì„± (ì–´í…ì…˜ ì¶œë ¥ê¹Œì§€)
        attention_model = Model(
            inputs=self.model.input,
            outputs=attention_layer.output
        )

        # ì–´í…ì…˜ ì¶œë ¥ ê³„ì‚°
        attention_output = attention_model.predict(X_sample, verbose=0)

        return attention_output

    def print_model_summary(self):
        """ëª¨ë¸ êµ¬ì¡° ìš”ì•½ ì¶œë ¥"""
        if self.model is None:
            print("âš ï¸ ëª¨ë¸ì´ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        print("\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¡° ìš”ì•½:")
        print("="*50)
        self.model.summary()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬í˜„ ì‹œì‘")
    print("="*60)

    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ê°ì²´ ìƒì„±
    sequence_length = 20
    hybrid_model = LSTMTransformerHybrid(
        sequence_length=sequence_length,
        num_features=5,
        lstm_units=64,
        attention_heads=8,
        d_model=128
    )

    # 1. ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
    print("\nğŸ“‹ 1ë‹¨ê³„: ì •ì±… ì‹œê³„ì—´ ë°ì´í„° ìƒì„±")
    data = hybrid_model.generate_policy_timeseries(n_samples=2000, noise_level=0.1)

    # ë°ì´í„° í™•ì¸
    print(f"\nğŸ“Š ë°ì´í„° ìš”ì•½:")
    print(data.describe())

    # 2. ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
    print("\nâš™ï¸ 2ë‹¨ê³„: ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±")
    X, y = hybrid_model.create_sequences(data, target_col='ì •ì±…íš¨ê³¼ì§€ìˆ˜')

    # 3. ë°ì´í„° ë¶„í• 
    print("\nğŸ“‚ 3ë‹¨ê³„: ë°ì´í„° ë¶„í• ")
    train_size = int(0.7 * len(X))
    val_size = int(0.15 * len(X))

    X_train = X[:train_size]
    y_train = y[:train_size]
    X_val = X[train_size:train_size + val_size]
    y_val = y[train_size:train_size + val_size]
    X_test = X[train_size + val_size:]
    y_test = y[train_size + val_size:]

    print(f"   - í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"   - ê²€ì¦ ë°ì´í„°: {X_val.shape}")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}")

    # 4. ëª¨ë¸ êµ¬ì¶•
    print("\nğŸ—ï¸ 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì¶•")
    hybrid_model.build_hybrid_model()
    hybrid_model.print_model_summary()

    # 5. ëª¨ë¸ í•™ìŠµ
    print("\nğŸš€ 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ")
    hybrid_model.train_model(
        X_train, y_train,
        X_val, y_val,
        epochs=100,
        batch_size=32
    )

    # 6. ëª¨ë¸ í‰ê°€
    print("\nğŸ“Š 6ë‹¨ê³„: ëª¨ë¸ í‰ê°€")
    results = hybrid_model.evaluate_model(X_test, y_test)

    # 7. ê²°ê³¼ ì‹œê°í™”
    print("\nğŸ“ˆ 7ë‹¨ê³„: ê²°ê³¼ ì‹œê°í™”")
    hybrid_model.plot_training_history()
    hybrid_model.plot_predictions(y_test, results['predictions'])

    # 8. ì–´í…ì…˜ ë¶„ì„ (ìƒ˜í”Œ)
    print("\nğŸ” 8ë‹¨ê³„: ì–´í…ì…˜ ë¶„ì„")
    sample_input = X_test[:5]  # 5ê°œ ìƒ˜í”Œ
    attention_weights = hybrid_model.get_attention_weights(sample_input)

    if attention_weights is not None:
        print(f"   - ì–´í…ì…˜ ì¶œë ¥ í˜•íƒœ: {attention_weights.shape}")
        print("   - ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì´ ì‹œê³„ì—´ íŒ¨í„´ì„ ì„±ê³µì ìœ¼ë¡œ í¬ì°©í–ˆìŠµë‹ˆë‹¤.")

    # 9. ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ¯ LSTM-Transformer í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬í˜„ ì™„ë£Œ!")
    print("="*60)
    print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥:")
    print(f"   - RMSE: {results['RMSE']:.6f}")
    print(f"   - MAE: {results['MAE']:.6f}")
    print(f"   - RÂ²: {results['RÂ²']:.6f}")

    print(f"\nğŸ—ï¸ ëª¨ë¸ êµ¬ì¡°:")
    print(f"   - ì´ íŒŒë¼ë¯¸í„°: {hybrid_model.model.count_params():,}")
    print(f"   - LSTM ìœ ë‹›: {hybrid_model.lstm_units}")
    print(f"   - ì–´í…ì…˜ í—¤ë“œ: {hybrid_model.attention_heads}")

    print("\nğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” practice/chapter05/outputs/ í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()