"""
ìµœì  ëª¨ë¸ ì„ íƒì„ ìœ„í•œ ì²´ê³„ì  ë°©ë²•ë¡ 
Author: AI Policy Analyst
Date: 2024

ëª©ì : ì •ì±… ë¶„ì„ì„ ìœ„í•œ ìµœì  ëª¨ë¸ì„ ê³¼í•™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” í”„ë ˆì„ì›Œí¬
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import make_scorer, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')
# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
if platform.system() == 'Windows':
    plt.rcParams['font.family'] = 'Malgun Gothic'
elif platform.system() == 'Darwin':  # macOS
    plt.rcParams['font.family'] = 'AppleGothic'
else:
    plt.rcParams['font.family'] = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False  # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€


class OptimalModelSelector:
    """
    ì •ì±… ë¶„ì„ì„ ìœ„í•œ ìµœì  ëª¨ë¸ ì„ íƒ í”„ë ˆì„ì›Œí¬
    """

    def __init__(self, objective='balanced'):
        """
        Parameters:
        -----------
        objective : str
            'explanation' - ì„¤ëª…ë ¥ ì¤‘ì‹¬
            'prediction' - ì˜ˆì¸¡ë ¥ ì¤‘ì‹¬
            'balanced' - ê· í˜• ì¶”êµ¬
            'speed' - ì‹¤ì‹œê°„ ì²˜ë¦¬ ì¤‘ì‹¬
        """
        self.objective = objective
        self.results = {}

        # ëª©ì ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
        self.weights = self._set_weights(objective)

        # í›„ë³´ ëª¨ë¸ ì •ì˜
        self.models = self._initialize_models()

    def _set_weights(self, objective):
        """ëª©ì ë³„ í‰ê°€ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ì„¤ì •"""
        weight_profiles = {
            'explanation': {
                'accuracy': 0.2,
                'explainability': 0.4,
                'simplicity': 0.2,
                'speed': 0.1,
                'robustness': 0.1
            },
            'prediction': {
                'accuracy': 0.5,
                'explainability': 0.1,
                'simplicity': 0.05,
                'speed': 0.15,
                'robustness': 0.2
            },
            'balanced': {
                'accuracy': 0.3,
                'explainability': 0.3,
                'simplicity': 0.15,
                'speed': 0.1,
                'robustness': 0.15
            },
            'speed': {
                'accuracy': 0.2,
                'explainability': 0.1,
                'simplicity': 0.2,
                'speed': 0.4,
                'robustness': 0.1
            }
        }
        return weight_profiles[objective]

    def _initialize_models(self):
        """í›„ë³´ ëª¨ë¸ ì´ˆê¸°í™”"""
        return {
            'Linear Regression': {
                'model': LinearRegression(),
                'explainability': 1.0,  # ì™„ì „ ì„¤ëª… ê°€ëŠ¥
                'simplicity': 1.0,
                'category': 'linear'
            },
            'Ridge Regression': {
                'model': Ridge(alpha=1.0),
                'explainability': 0.95,
                'simplicity': 0.95,
                'category': 'linear'
            },
            'Lasso Regression': {
                'model': Lasso(alpha=0.1),
                'explainability': 0.9,
                'simplicity': 0.9,
                'category': 'linear'
            },
            'Random Forest': {
                'model': RandomForestRegressor(n_estimators=100, random_state=42),
                'explainability': 0.6,  # íŠ¹ì„± ì¤‘ìš”ë„ë§Œ ì œê³µ
                'simplicity': 0.4,
                'category': 'ensemble'
            },
            'Gradient Boosting': {
                'model': GradientBoostingRegressor(n_estimators=100, random_state=42),
                'explainability': 0.5,
                'simplicity': 0.3,
                'category': 'ensemble'
            },
            'Neural Network': {
                'model': MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42),
                'explainability': 0.2,  # ë¸”ë™ë°•ìŠ¤
                'simplicity': 0.1,
                'category': 'deep'
            }
        }

    def evaluate_models(self, X, y, cv=5):
        """
        ëª¨ë“  ëª¨ë¸ì„ í‰ê°€í•˜ê³  ì ìˆ˜ ê³„ì‚°
        """
        print(f"\n{'='*60}")
        print(f"ëª©ì : {self.objective.upper()} ìµœì í™”")
        print(f"{'='*60}\n")

        # ë°ì´í„° ì •ê·œí™”
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        for name, config in self.models.items():
            print(f"í‰ê°€ ì¤‘: {name}")
            model = config['model']

            # 1. ì •í™•ë„ í‰ê°€ (êµì°¨ê²€ì¦)
            scores = cross_val_score(model, X_scaled, y, cv=cv,
                                    scoring='r2')
            accuracy = scores.mean()
            robustness = 1 - scores.std()  # ì•ˆì •ì„±

            # 2. ì†ë„ í‰ê°€
            import time
            start = time.time()
            model.fit(X_scaled, y)
            train_time = time.time() - start

            start = time.time()
            _ = model.predict(X_scaled[:100])
            pred_time = time.time() - start

            # ì†ë„ ì ìˆ˜ (ì—­ìˆ˜ ì •ê·œí™”)
            speed_score = 1 / (1 + train_time + pred_time * 10)

            # 3. ì¢…í•© ì ìˆ˜ ê³„ì‚°
            total_score = (
                self.weights['accuracy'] * accuracy +
                self.weights['explainability'] * config['explainability'] +
                self.weights['simplicity'] * config['simplicity'] +
                self.weights['speed'] * speed_score +
                self.weights['robustness'] * robustness
            )

            # ê²°ê³¼ ì €ì¥
            self.results[name] = {
                'accuracy': accuracy,
                'explainability': config['explainability'],
                'simplicity': config['simplicity'],
                'speed': speed_score,
                'robustness': robustness,
                'total_score': total_score,
                'train_time': train_time,
                'category': config['category']
            }

            print(f"  - RÂ² Score: {accuracy:.4f}")
            print(f"  - ì¢…í•© ì ìˆ˜: {total_score:.4f}\n")

    def find_pareto_optimal(self):
        """
        íŒŒë ˆí†  ìµœì  ëª¨ë¸ë“¤ ì°¾ê¸°
        (ì •í™•ë„ì™€ ì„¤ëª…ë ¥ì˜ trade-off)
        """
        pareto_models = []

        for name1, metrics1 in self.results.items():
            is_dominated = False

            for name2, metrics2 in self.results.items():
                if name1 == name2:
                    continue

                # name2ê°€ name1ì„ ì§€ë°°í•˜ëŠ”ì§€ í™•ì¸
                if (metrics2['accuracy'] > metrics1['accuracy'] and
                    metrics2['explainability'] >= metrics1['explainability']) or \
                   (metrics2['accuracy'] >= metrics1['accuracy'] and
                    metrics2['explainability'] > metrics1['explainability']):
                    is_dominated = True
                    break

            if not is_dominated:
                pareto_models.append(name1)

        return pareto_models

    def recommend_model(self):
        """
        ìµœì¢… ëª¨ë¸ ì¶”ì²œ
        """
        # 1. ì¢…í•© ì ìˆ˜ ìµœê³  ëª¨ë¸
        best_overall = max(self.results.items(),
                          key=lambda x: x[1]['total_score'])

        # 2. íŒŒë ˆí†  ìµœì  ëª¨ë¸ë“¤
        pareto_models = self.find_pareto_optimal()

        # 3. ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸
        category_best = {}
        for name, metrics in self.results.items():
            cat = metrics['category']
            if cat not in category_best or \
               metrics['total_score'] > category_best[cat][1]:
                category_best[cat] = (name, metrics['total_score'])

        print("\n" + "="*60)
        print("ğŸ“Š ëª¨ë¸ ì„ íƒ ê²°ê³¼")
        print("="*60)

        print(f"\nâœ… ì¢…í•© ìµœì  ëª¨ë¸: {best_overall[0]}")
        print(f"   ì¢…í•© ì ìˆ˜: {best_overall[1]['total_score']:.4f}")
        print(f"   ì •í™•ë„: {best_overall[1]['accuracy']:.4f}")
        print(f"   ì„¤ëª…ë ¥: {best_overall[1]['explainability']:.2f}")

        print(f"\nğŸ¯ íŒŒë ˆí†  ìµœì  ëª¨ë¸ë“¤ (ì •í™•ë„-ì„¤ëª…ë ¥):")
        for model in pareto_models:
            print(f"   - {model}: ì •í™•ë„={self.results[model]['accuracy']:.3f}, "
                  f"ì„¤ëª…ë ¥={self.results[model]['explainability']:.2f}")

        print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ ìµœê³  ëª¨ë¸:")
        for cat, (model, score) in category_best.items():
            print(f"   - {cat}: {model} (ì ìˆ˜: {score:.4f})")

        # ìµœì¢… ì¶”ì²œ
        print(f"\nğŸ’¡ ìµœì¢… ì¶”ì²œ:")
        if self.objective == 'explanation':
            # ì„¤ëª…ë ¥ ìš°ì„ ì‹œ íŒŒë ˆí†  ìµœì  ì¤‘ ì„¤ëª…ë ¥ ë†’ì€ ëª¨ë¸
            pareto_explain = [(m, self.results[m]['explainability'])
                             for m in pareto_models]
            recommended = max(pareto_explain, key=lambda x: x[1])[0]
        else:
            recommended = best_overall[0]

        print(f"   ëª©ì  '{self.objective}'ì— ê°€ì¥ ì í•©í•œ ëª¨ë¸: {recommended}")

        return recommended, self.results

    def visualize_results(self):
        """
        ëª¨ë¸ ì„ íƒ ê²°ê³¼ ì‹œê°í™”
        """
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # 1. ì¢…í•© ì ìˆ˜ ë¹„êµ
        models = list(self.results.keys())
        scores = [self.results[m]['total_score'] for m in models]

        axes[0, 0].barh(models, scores, color='steelblue')
        axes[0, 0].set_xlabel('ì¢…í•© ì ìˆ˜')
        axes[0, 0].set_title(f'ëª¨ë¸ë³„ ì¢…í•© ì ìˆ˜ ({self.objective} ìµœì í™”)')
        axes[0, 0].axvline(x=max(scores), color='red', linestyle='--', alpha=0.5)

        # 2. ì •í™•ë„ vs ì„¤ëª…ë ¥ (íŒŒë ˆí†  í”„ë¡ íŠ¸)
        accuracy = [self.results[m]['accuracy'] for m in models]
        explainability = [self.results[m]['explainability'] for m in models]

        pareto_models = self.find_pareto_optimal()

        axes[0, 1].scatter(accuracy, explainability, s=100, alpha=0.6)
        for i, model in enumerate(models):
            color = 'red' if model in pareto_models else 'black'
            weight = 'bold' if model in pareto_models else 'normal'
            axes[0, 1].annotate(model, (accuracy[i], explainability[i]),
                               fontsize=8, color=color, weight=weight)

        axes[0, 1].set_xlabel('ì •í™•ë„ (RÂ² Score)')
        axes[0, 1].set_ylabel('ì„¤ëª…ë ¥')
        axes[0, 1].set_title('ì •í™•ë„ vs ì„¤ëª…ë ¥ (ë¹¨ê°„ìƒ‰: íŒŒë ˆí†  ìµœì )')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. ë‹¤ì°¨ì› í‰ê°€ ë ˆì´ë” ì°¨íŠ¸
        categories = ['ì •í™•ë„', 'ì„¤ëª…ë ¥', 'ë‹¨ìˆœì„±', 'ì†ë„', 'ê°•ê±´ì„±']

        # ìƒìœ„ 3ê°œ ëª¨ë¸ë§Œ í‘œì‹œ
        top_models = sorted(self.results.items(),
                          key=lambda x: x[1]['total_score'],
                          reverse=True)[:3]

        angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]

        ax = plt.subplot(223, projection='polar')

        for name, metrics in top_models:
            values = [
                metrics['accuracy'],
                metrics['explainability'],
                metrics['simplicity'],
                metrics['speed'],
                metrics['robustness']
            ]
            values += values[:1]
            ax.plot(angles, values, 'o-', linewidth=2, label=name)
            ax.fill(angles, values, alpha=0.25)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1)
        ax.set_title('Top 3 ëª¨ë¸ ë‹¤ì°¨ì› ë¹„êµ')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)

        # 4. ê°€ì¤‘ì¹˜ ì˜í–¥ ë¶„ì„
        weight_keys = list(self.weights.keys())
        weight_values = list(self.weights.values())

        axes[1, 1].pie(weight_values, labels=weight_keys, autopct='%1.1f%%',
                      startangle=90, colors=plt.cm.Set3.colors)
        axes[1, 1].set_title(f'í‰ê°€ ê¸°ì¤€ ê°€ì¤‘ì¹˜ ({self.objective} ëª¨ë“œ)')

        plt.tight_layout()

        # ì €ì¥
        output_dir = 'c:/practice/chap/chapter05/outputs'
        import os
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(f'{output_dir}/model_selection_{self.objective}.png',
                   dpi=300, bbox_inches='tight')
        plt.show()

        return fig


def demonstrate_model_selection():
    """
    ë‹¤ì–‘í•œ ë°ì´í„° íŒ¨í„´ì—ì„œ ìµœì  ëª¨ë¸ ì„ íƒ ì‹œì—°
    """
    np.random.seed(42)

    # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë°ì´í„° ìƒì„±
    scenarios = {
        'ì„ í˜• íŒ¨í„´': lambda X: 2*X[:, 0] + 3*X[:, 1] - X[:, 2],
        'ë¹„ì„ í˜• íŒ¨í„´': lambda X: np.sin(X[:, 0]) + X[:, 1]**2 + np.log(np.abs(X[:, 2]) + 1),
        'ë³µì¡í•œ ìƒí˜¸ì‘ìš©': lambda X: X[:, 0]*X[:, 1] + np.exp(-X[:, 2]) + X[:, 3]**3
    }

    n_samples = 1000
    n_features = 10

    results_summary = {}

    for scenario_name, target_func in scenarios.items():
        print(f"\n{'='*60}")
        print(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario_name}")
        print(f"{'='*60}")

        # ë°ì´í„° ìƒì„±
        X = np.random.randn(n_samples, n_features)
        y = target_func(X) + np.random.randn(n_samples) * 0.1

        # ê° ëª©ì ë³„ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
        objectives = ['explanation', 'prediction', 'balanced']
        scenario_results = {}

        for obj in objectives:
            selector = OptimalModelSelector(objective=obj)
            selector.evaluate_models(X, y)
            recommended, all_results = selector.recommend_model()

            scenario_results[obj] = {
                'recommended': recommended,
                'score': all_results[recommended]['total_score'],
                'accuracy': all_results[recommended]['accuracy']
            }

            # ì²« ë²ˆì§¸ ëª©ì ì— ëŒ€í•´ì„œë§Œ ì‹œê°í™”
            if obj == 'balanced':
                selector.visualize_results()

        results_summary[scenario_name] = scenario_results

    # ìµœì¢… ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½")
    print("="*60)

    summary_df = pd.DataFrame()
    for scenario, objectives in results_summary.items():
        for obj, metrics in objectives.items():
            row = {
                'ì‹œë‚˜ë¦¬ì˜¤': scenario,
                'ëª©ì ': obj,
                'ì¶”ì²œ ëª¨ë¸': metrics['recommended'],
                'ì •í™•ë„': f"{metrics['accuracy']:.4f}",
                'ì¢…í•© ì ìˆ˜': f"{metrics['score']:.4f}"
            }
            summary_df = pd.concat([summary_df, pd.DataFrame([row])],
                                  ignore_index=True)

    print(summary_df.to_string(index=False))

    # ê²°ê³¼ ì €ì¥
    output_dir = 'c:/practice/chap/chapter05/outputs'
    os.makedirs(output_dir, exist_ok=True)
    summary_df.to_csv(f'{output_dir}/model_selection_summary.csv',
                     index=False, encoding='utf-8-sig')

    print(f"\nâœ… ê²°ê³¼ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return results_summary


if __name__ == "__main__":
    print("ğŸš€ ìµœì  ëª¨ë¸ ì„ íƒ í”„ë ˆì„ì›Œí¬ ì‹œì‘")
    results = demonstrate_model_selection()
    print("\nâœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")