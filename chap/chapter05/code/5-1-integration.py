#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„
ML/DL í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì˜ˆì œ
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10

class MLDLIntegrationPipeline:
    """ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ í†µí•© íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""

    def __init__(self, config=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Parameters:
        config (dict): íŒŒì´í”„ë¼ì¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        self.scaler = StandardScaler()
        self.ml_models = {}
        self.dl_model = None
        self.ensemble_model = None
        self.feature_names = None

    def generate_sample_data(self, n_samples=1000, n_features=5, noise=0.1):
        """
        ì‹œë®¬ë ˆì´ì…˜ ì •ì±… ë°ì´í„° ìƒì„±
        â€» ë³¸ ë°ì´í„°ëŠ” êµìœ¡ ëª©ì ì˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤

        Parameters:
        n_samples (int): ìƒ˜í”Œ ìˆ˜
        n_features (int): íŠ¹ì„± ìˆ˜
        noise (float): ë…¸ì´ì¦ˆ ë ˆë²¨

        Returns:
        tuple: (X, y) íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë°ì´í„°
        """
        np.random.seed(42)

        # ì •ì±… ê´€ë ¨ íŠ¹ì„± ìƒì„± (ì •ê·œí™”ëœ ê°’)
        X = np.random.randn(n_samples, n_features)

        # ë¹„ì„ í˜• ê´€ê³„ë¥¼ ê°€ì§„ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        y = (2 * X[:, 0] +
             1.5 * X[:, 1] ** 2 +
             0.8 * X[:, 2] * X[:, 3] +
             0.5 * np.sin(X[:, 4]) +
             noise * np.random.randn(n_samples))

        # DataFrameìœ¼ë¡œ ë³€í™˜
        feature_names = [f'Feature_{i+1}' for i in range(n_features)]
        X_df = pd.DataFrame(X, columns=feature_names)

        self.feature_names = feature_names

        print(f"âœ… Simulation data generated: {n_samples} samples, {n_features} features")
        return X_df, y

    def save_data(self, X, y, base_path='../data/'):
        """
        ìƒì„±ëœ ë°ì´í„°ë¥¼ í†µí•© CSV íŒŒì¼ë¡œ ì €ì¥

        Parameters:
        X (DataFrame): íŠ¹ì„± ë°ì´í„°
        y (array): íƒ€ê²Ÿ ë°ì´í„°
        base_path (str): ì €ì¥í•  ê¸°ë³¸ ê²½ë¡œ
        """
        # ì €ì¥ ê²½ë¡œ í™•ì¸ ë° ìƒì„±
        if not os.path.exists(base_path):
            os.makedirs(base_path)

        # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿì„ í•¨ê»˜ ì €ì¥ (í†µí•© ë°ì´í„°)
        data_combined = X.copy()
        data_combined['Target'] = y
        csv_path = os.path.join(base_path, f'integration_data_{timestamp}.csv')
        data_combined.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ Integration data saved: {csv_path}")

        print(f"\nâœ… Data saved to {base_path} folder.")

    def preprocess_data(self, X, y):
        """
        ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰

        Parameters:
        X (DataFrame): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜

        Returns:
        tuple: ì „ì²˜ë¦¬ëœ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°
        """
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        if X.isnull().sum().sum() > 0:
            X_processed = X.fillna(X.median(numeric_only=True))
            print("ğŸ“‹ Missing values filled with median")
        else:
            X_processed = X.copy()

        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (ë§Œì•½ ìˆë‹¤ë©´)
        categorical_cols = X_processed.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            le = LabelEncoder()
            X_processed[col] = le.fit_transform(X_processed[col])
            print(f"ğŸ“‹ {col} categorical encoding completed")

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y, test_size=0.2, random_state=42
        )

        # ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # DataFrameìœ¼ë¡œ ë³€í™˜ (íŠ¹ì„±ëª… ìœ ì§€)
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_processed.columns)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_processed.columns)

        print("âœ… Data preprocessing completed")
        print(f"   - Training data: {X_train_scaled.shape}")
        print(f"   - Test data: {X_test_scaled.shape}")

        return X_train_scaled, X_test_scaled, y_train, y_test

    def build_ml_models(self, X_train, y_train):
        """
        ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

        Parameters:
        X_train (DataFrame): í•™ìŠµìš© íŠ¹ì„± ë°ì´í„°
        y_train (array): í•™ìŠµìš© íƒ€ê²Ÿ ë°ì´í„°

        Returns:
        dict: í•™ìŠµëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤
        """
        # Random Forest ëª¨ë¸
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        self.ml_models['random_forest'] = rf_model

        print("âœ… Random Forest model training completed")

        return self.ml_models

    def build_dl_model(self, input_shape, sequence_length=10):
        """
        ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•

        Parameters:
        input_shape (int): ì…ë ¥ íŠ¹ì„± ìˆ˜
        sequence_length (int): ì‹œí€€ìŠ¤ ê¸¸ì´ (LSTMìš©)

        Returns:
        Model: êµ¬ì¶•ëœ ë”¥ëŸ¬ë‹ ëª¨ë¸
        """
        model = Sequential([
            Dense(64, activation='relu', input_shape=(input_shape,)),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(16, activation='relu'),
            Dense(1)
        ])

        model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )

        self.dl_model = model

        print("âœ… Deep learning model built")
        print(f"   - Total parameters: {model.count_params():,}")

        return model

    def train_dl_model(self, X_train, y_train, X_val, y_val, epochs=50):
        """
        ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ

        Parameters:
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜

        Returns:
        History: í•™ìŠµ ì´ë ¥
        """
        # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )

        # ëª¨ë¸ í•™ìŠµ
        history = self.dl_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=1
        )

        print("âœ… Deep learning model training completed")

        return history

    def create_ensemble(self):
        """
        ì•™ìƒë¸” ëª¨ë¸ ìƒì„±

        Returns:
        VotingRegressor: ì•™ìƒë¸” ëª¨ë¸
        """
        if not self.ml_models or self.dl_model is None:
            raise ValueError("Please train individual models first")

        # ML ëª¨ë¸ë“¤ì„ ìœ„í•œ VotingRegressor
        ml_estimators = [(name, model) for name, model in self.ml_models.items()]

        self.ensemble_model = VotingRegressor(ml_estimators)

        print("âœ… Ensemble model created")

        return self.ensemble_model

    def evaluate_models(self, X_test, y_test):
        """
        ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€

        Parameters:
        X_test (DataFrame): í…ŒìŠ¤íŠ¸ íŠ¹ì„± ë°ì´í„°
        y_test (array): í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°

        Returns:
        dict: í‰ê°€ ê²°ê³¼
        """
        results = {}

        # ML ëª¨ë¸ í‰ê°€
        for name, model in self.ml_models.items():
            predictions = model.predict(X_test)

            mse = mean_squared_error(y_test, predictions)
            mae = mean_absolute_error(y_test, predictions)
            r2 = r2_score(y_test, predictions)

            results[name] = {
                'MSE': mse,
                'MAE': mae,
                'RMSE': np.sqrt(mse),
                'RÂ²': r2
            }

        # DL ëª¨ë¸ í‰ê°€
        if self.dl_model is not None:
            dl_predictions = self.dl_model.predict(X_test, verbose=0)
            dl_mse = mean_squared_error(y_test, dl_predictions)

            results['deep_learning'] = {
                'MSE': dl_mse,
                'MAE': mean_absolute_error(y_test, dl_predictions),
                'RMSE': np.sqrt(dl_mse),
                'RÂ²': r2_score(y_test, dl_predictions)
            }

        # ì•™ìƒë¸” ëª¨ë¸ í‰ê°€ (ML ëª¨ë¸ë“¤ë§Œ)
        if self.ensemble_model is not None:
            ensemble_pred = self.ensemble_model.predict(X_test)
            ensemble_mse = mean_squared_error(y_test, ensemble_pred)

            results['ensemble'] = {
                'MSE': ensemble_mse,
                'MAE': mean_absolute_error(y_test, ensemble_pred),
                'RMSE': np.sqrt(ensemble_mse),
                'RÂ²': r2_score(y_test, ensemble_pred)
            }

        print("ğŸ“Š All models evaluated")

        return results

    def save_results(self, results, X_test, y_test, base_path='../data/'):
        """
        í‰ê°€ ê²°ê³¼ë¥¼ ë°ì´í„° í´ë”ì— ì €ì¥

        Parameters:
        results (dict): í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        X_test (DataFrame): í…ŒìŠ¤íŠ¸ ë°ì´í„°
        y_test (array): í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°
        base_path (str): ì €ì¥í•  ê¸°ë³¸ ê²½ë¡œ
        """
        if not os.path.exists(base_path):
            os.makedirs(base_path)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        results_df = pd.DataFrame(results).T
        csv_path = os.path.join(base_path, f'model_metrics_{timestamp}.csv')
        results_df.to_csv(csv_path, encoding='utf-8-sig')
        print(f"ğŸ“ Model metrics saved: {csv_path}")

        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        predictions_data = {'actual': y_test}

        for name, model in self.ml_models.items():
            predictions_data[f'pred_{name}'] = model.predict(X_test)

        if self.dl_model is not None:
            predictions_data['pred_deep_learning'] = self.dl_model.predict(X_test, verbose=0).flatten()

        if self.ensemble_model is not None:
            predictions_data['pred_ensemble'] = self.ensemble_model.predict(X_test)

        predictions_df = pd.DataFrame(predictions_data)
        pred_csv_path = os.path.join(base_path, f'predictions_{timestamp}.csv')
        predictions_df.to_csv(pred_csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“ Predictions saved: {pred_csv_path}")

        print(f"\nâœ… Results saved to {base_path} folder.")

    def print_results(self, results):
        """
        í‰ê°€ ê²°ê³¼ ì¶œë ¥

        Parameters:
        results (dict): í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*60)
        print("ğŸ“Š Model Performance Comparison")
        print("="*60)

        for model_name, metrics in results.items():
            print(f"\nğŸ”¹ {model_name.upper()} Model:")
            print(f"  MSE:  {metrics['MSE']:.4f}")
            print(f"  MAE:  {metrics['MAE']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  RÂ²:   {metrics['RÂ²']:.4f}")

    def plot_results(self, results, save_path='../outputs/model_comparison.png'):
        """
        ê²°ê³¼ ì‹œê°í™”

        Parameters:
        results (dict): í‰ê°€ ê²°ê³¼
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        metrics = ['MSE', 'MAE', 'RMSE', 'RÂ²']
        models = list(results.keys())

        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        axes = axes.ravel()

        for i, metric in enumerate(metrics):
            values = [results[model][metric] for model in models]

            bars = axes[i].bar(models, values, alpha=0.7)
            axes[i].set_title(f'{metric} Comparison')
            axes[i].set_ylabel(metric)
            axes[i].tick_params(axis='x', rotation=45)

            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ Visualization saved: {save_path}")

def main():
    """Main execution function"""
    print("ğŸš€ Starting ML/DL Integration Pipeline")
    print("="*60)

    # Initialize pipeline
    pipeline = MLDLIntegrationPipeline()

    # 1. Generate data
    print("\nğŸ“‹ Step 1: Data Generation")
    X, y = pipeline.generate_sample_data(n_samples=1000, n_features=5)

    # Save data
    print("\nğŸ’¾ Saving data...")
    pipeline.save_data(X, y, base_path='../data/')

    # 2. Preprocess data
    print("\nâš™ï¸ Step 2: Data Preprocessing")
    X_train, X_test, y_train, y_test = pipeline.preprocess_data(X, y)

    # Split validation data
    X_train_split, X_val, y_train_split, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )

    # 3. Build ML models
    print("\nğŸ¤– Step 3: Building ML Models")
    pipeline.build_ml_models(X_train, y_train)

    # 4. Build and train DL model
    print("\nğŸ§  Step 4: Building and Training DL Model")
    pipeline.build_dl_model(X_train.shape[1])
    pipeline.train_dl_model(X_train_split, y_train_split, X_val, y_val, epochs=50)

    # 5. Create ensemble
    print("\nğŸ¯ Step 5: Creating Ensemble Model")
    pipeline.create_ensemble()
    pipeline.ensemble_model.fit(X_train, y_train)

    # 6. Evaluate models
    print("\nğŸ“Š Step 6: Model Performance Evaluation")
    results = pipeline.evaluate_models(X_test, y_test)

    # 7. Display and visualize results
    pipeline.print_results(results)
    pipeline.plot_results(results)

    # 8. Save results
    print("\nğŸ’¾ Saving evaluation results...")
    pipeline.save_results(results, X_test, y_test, base_path='../data/')

    print("\nâœ… Integration Pipeline Completed!")
    print("ğŸ“ Generated data saved in: practice/chapter05/data/")
    print("ğŸ“ Visualization saved in: practice/chapter05/outputs/")

if __name__ == "__main__":
    main()