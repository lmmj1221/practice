#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì œ5ì¥: ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬í˜„
ML/DL í†µí•© íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì˜ˆì œ
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Arial'
plt.rcParams['font.size'] = 10

class MLDLIntegrationPipeline:
    """ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ í†µí•© íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤"""

    def __init__(self, config=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”

        Parameters:
        config (dict): íŒŒì´í”„ë¼ì¸ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or {}
        self.scaler = StandardScaler()
        self.ml_models = {}
        self.dl_model = None
        self.ensemble_model = None
        self.feature_names = None

    def generate_sample_data(self, n_samples=1000, n_features=5, noise=0.1):
        """
        ì‹œë®¬ë ˆì´ì…˜ ì •ì±… ë°ì´í„° ìƒì„±
        â€» ë³¸ ë°ì´í„°ëŠ” êµìœ¡ ëª©ì ì˜ ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ì…ë‹ˆë‹¤

        Parameters:
        n_samples (int): ìƒ˜í”Œ ìˆ˜
        n_features (int): íŠ¹ì„± ìˆ˜
        noise (float): ë…¸ì´ì¦ˆ ë ˆë²¨

        Returns:
        tuple: (X, y) íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë°ì´í„°
        """
        np.random.seed(42)

        # ì •ì±… ê´€ë ¨ íŠ¹ì„± ìƒì„± (ì •ê·œí™”ëœ ê°’)
        X = np.random.randn(n_samples, n_features)

        # ë¹„ì„ í˜• ê´€ê³„ë¥¼ ê°€ì§„ íƒ€ê²Ÿ ë³€ìˆ˜ ìƒì„±
        y = (2 * X[:, 0] +
             1.5 * X[:, 1] ** 2 +
             0.8 * X[:, 2] * X[:, 3] +
             0.5 * np.sin(X[:, 4]) +
             noise * np.random.randn(n_samples))

        # DataFrameìœ¼ë¡œ ë³€í™˜
        feature_names = [f'ì •ì±…ë³€ìˆ˜_{i+1}' for i in range(n_features)]
        X_df = pd.DataFrame(X, columns=feature_names)

        self.feature_names = feature_names

        print(f"âœ… ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì™„ë£Œ: {n_samples}ê°œ ìƒ˜í”Œ, {n_features}ê°œ íŠ¹ì„±")
        return X_df, y

    def preprocess_data(self, X, y):
        """
        ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜í–‰

        Parameters:
        X (DataFrame): ì…ë ¥ íŠ¹ì„±
        y (array): íƒ€ê²Ÿ ë³€ìˆ˜

        Returns:
        tuple: ì „ì²˜ë¦¬ëœ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°
        """
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        if X.isnull().sum().sum() > 0:
            X_processed = X.fillna(X.median(numeric_only=True))
            print("ğŸ“‹ ê²°ì¸¡ê°’ ì¤‘ìœ„ìˆ˜ë¡œ ëŒ€ì²´")
        else:
            X_processed = X.copy()

        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (ë§Œì•½ ìˆë‹¤ë©´)
        categorical_cols = X_processed.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            le = LabelEncoder()
            X_processed[col] = le.fit_transform(X_processed[col])
            print(f"ğŸ“‹ {col} ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ì™„ë£Œ")

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_processed, y, test_size=0.2, random_state=42
        )

        # ì •ê·œí™”
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # DataFrameìœ¼ë¡œ ë³€í™˜ (íŠ¹ì„±ëª… ìœ ì§€)
        X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_processed.columns)
        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_processed.columns)

        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")
        print(f"   - í•™ìŠµ ë°ì´í„°: {X_train_scaled.shape}")
        print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test_scaled.shape}")

        return X_train_scaled, X_test_scaled, y_train, y_test

    def build_ml_models(self, X_train, y_train):
        """
        ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ

        Parameters:
        X_train (DataFrame): í•™ìŠµìš© íŠ¹ì„± ë°ì´í„°
        y_train (array): í•™ìŠµìš© íƒ€ê²Ÿ ë°ì´í„°

        Returns:
        dict: í•™ìŠµëœ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤
        """
        # Random Forest ëª¨ë¸
        rf_model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
        rf_model.fit(X_train, y_train)
        self.ml_models['random_forest'] = rf_model

        print("âœ… Random Forest ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

        return self.ml_models

    def build_dl_model(self, input_shape, sequence_length=10):
        """
        ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•

        Parameters:
        input_shape (int): ì…ë ¥ íŠ¹ì„± ìˆ˜
        sequence_length (int): ì‹œí€€ìŠ¤ ê¸¸ì´ (LSTMìš©)

        Returns:
        Model: êµ¬ì¶•ëœ ë”¥ëŸ¬ë‹ ëª¨ë¸
        """
        model = Sequential([
            Dense(64, activation='relu', input_shape=(input_shape,)),
            Dropout(0.3),
            Dense(32, activation='relu'),
            Dropout(0.3),
            Dense(16, activation='relu'),
            Dense(1)
        ])

        model.compile(
            optimizer='adam',
            loss='mse',
            metrics=['mae']
        )

        self.dl_model = model

        print("âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        print(f"   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model.count_params():,}")

        return model

    def train_dl_model(self, X_train, y_train, X_val, y_val, epochs=50):
        """
        ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ

        Parameters:
        X_train, y_train: í•™ìŠµ ë°ì´í„°
        X_val, y_val: ê²€ì¦ ë°ì´í„°
        epochs (int): í•™ìŠµ ì—í¬í¬ ìˆ˜

        Returns:
        History: í•™ìŠµ ì´ë ¥
        """
        # ì¡°ê¸° ì¢…ë£Œ ì½œë°±
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        )

        # ëª¨ë¸ í•™ìŠµ
        history = self.dl_model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=32,
            callbacks=[early_stopping],
            verbose=1
        )

        print("âœ… ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

        return history

    def create_ensemble(self):
        """
        ì•™ìƒë¸” ëª¨ë¸ ìƒì„±

        Returns:
        VotingRegressor: ì•™ìƒë¸” ëª¨ë¸
        """
        if not self.ml_models or self.dl_model is None:
            raise ValueError("ë¨¼ì € ê°œë³„ ëª¨ë¸ë“¤ì„ í•™ìŠµí•´ì£¼ì„¸ìš”")

        # ML ëª¨ë¸ë“¤ì„ ìœ„í•œ VotingRegressor
        ml_estimators = [(name, model) for name, model in self.ml_models.items()]

        self.ensemble_model = VotingRegressor(ml_estimators)

        print("âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì™„ë£Œ")

        return self.ensemble_model

    def evaluate_models(self, X_test, y_test):
        """
        ëª¨ë“  ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€

        Parameters:
        X_test (DataFrame): í…ŒìŠ¤íŠ¸ íŠ¹ì„± ë°ì´í„°
        y_test (array): í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ ë°ì´í„°

        Returns:
        dict: í‰ê°€ ê²°ê³¼
        """
        results = {}

        # ML ëª¨ë¸ í‰ê°€
        for name, model in self.ml_models.items():
            predictions = model.predict(X_test)

            mse = mean_squared_error(y_test, predictions)
            mae = mean_absolute_error(y_test, predictions)
            r2 = r2_score(y_test, predictions)

            results[name] = {
                'MSE': mse,
                'MAE': mae,
                'RMSE': np.sqrt(mse),
                'RÂ²': r2
            }

        # DL ëª¨ë¸ í‰ê°€
        if self.dl_model is not None:
            dl_predictions = self.dl_model.predict(X_test, verbose=0)
            dl_mse = mean_squared_error(y_test, dl_predictions)

            results['deep_learning'] = {
                'MSE': dl_mse,
                'MAE': mean_absolute_error(y_test, dl_predictions),
                'RMSE': np.sqrt(dl_mse),
                'RÂ²': r2_score(y_test, dl_predictions)
            }

        # ì•™ìƒë¸” ëª¨ë¸ í‰ê°€ (ML ëª¨ë¸ë“¤ë§Œ)
        if self.ensemble_model is not None:
            ensemble_pred = self.ensemble_model.predict(X_test)
            ensemble_mse = mean_squared_error(y_test, ensemble_pred)

            results['ensemble'] = {
                'MSE': ensemble_mse,
                'MAE': mean_absolute_error(y_test, ensemble_pred),
                'RMSE': np.sqrt(ensemble_mse),
                'RÂ²': r2_score(y_test, ensemble_pred)
            }

        print("ğŸ“Š ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì™„ë£Œ")

        return results

    def print_results(self, results):
        """
        í‰ê°€ ê²°ê³¼ ì¶œë ¥

        Parameters:
        results (dict): í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "="*60)
        print("ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*60)

        for model_name, metrics in results.items():
            print(f"\nğŸ”¹ {model_name.upper()} ëª¨ë¸:")
            print(f"  MSE:  {metrics['MSE']:.4f}")
            print(f"  MAE:  {metrics['MAE']:.4f}")
            print(f"  RMSE: {metrics['RMSE']:.4f}")
            print(f"  RÂ²:   {metrics['RÂ²']:.4f}")

    def plot_results(self, results, save_path='../outputs/model_comparison.png'):
        """
        ê²°ê³¼ ì‹œê°í™”

        Parameters:
        results (dict): í‰ê°€ ê²°ê³¼
        save_path (str): ì €ì¥ ê²½ë¡œ
        """
        metrics = ['MSE', 'MAE', 'RMSE', 'RÂ²']
        models = list(results.keys())

        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        axes = axes.ravel()

        for i, metric in enumerate(metrics):
            values = [results[model][metric] for model in models]

            bars = axes[i].bar(models, values, alpha=0.7)
            axes[i].set_title(f'{metric} ë¹„êµ')
            axes[i].set_ylabel(metric)
            axes[i].tick_params(axis='x', rotation=45)

            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, values):
                height = bar.get_height()
                axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                           f'{value:.3f}', ha='center', va='bottom')

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ì €ì¥: {save_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("="*60)

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = MLDLIntegrationPipeline()

    # 1. ë°ì´í„° ìƒì„±
    print("\nğŸ“‹ 1ë‹¨ê³„: ë°ì´í„° ìƒì„±")
    X, y = pipeline.generate_sample_data(n_samples=1000, n_features=5)

    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    print("\nâš™ï¸ 2ë‹¨ê³„: ë°ì´í„° ì „ì²˜ë¦¬")
    X_train, X_test, y_train, y_test = pipeline.preprocess_data(X, y)

    # ê²€ì¦ ë°ì´í„° ë¶„í• 
    X_train_split, X_val, y_train_split, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42
    )

    # 3. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•
    print("\nğŸ¤– 3ë‹¨ê³„: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶•")
    pipeline.build_ml_models(X_train, y_train)

    # 4. ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ
    print("\nğŸ§  4ë‹¨ê³„: ë”¥ëŸ¬ë‹ ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ")
    pipeline.build_dl_model(X_train.shape[1])
    pipeline.train_dl_model(X_train_split, y_train_split, X_val, y_val, epochs=50)

    # 5. ì•™ìƒë¸” ëª¨ë¸ ìƒì„±
    print("\nğŸ¯ 5ë‹¨ê³„: ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
    pipeline.create_ensemble()
    pipeline.ensemble_model.fit(X_train, y_train)

    # 6. ëª¨ë¸ í‰ê°€
    print("\nğŸ“Š 6ë‹¨ê³„: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
    results = pipeline.evaluate_models(X_test, y_test)

    # 7. ê²°ê³¼ ì¶œë ¥ ë° ì‹œê°í™”
    pipeline.print_results(results)
    pipeline.plot_results(results)

    print("\nâœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
    print("ğŸ“ ìƒì„¸ ê²°ê³¼ëŠ” practice/chapter05/outputs/ í´ë”ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()