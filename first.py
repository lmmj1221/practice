"""
ì œ3ì¥: ë”¥ëŸ¬ë‹ ê¸°ì´ˆì™€ ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡
Complete implementation with all sections
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
import os

warnings.filterwarnings('ignore')

# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
os.makedirs('output', exist_ok=True)

# =====================================================
# Section 3.1: ë”¥ëŸ¬ë‹ ê¸°ì´ˆì™€ ì‹ ê²½ë§
# =====================================================

print("=" * 60)
print("Section 3.1: ë”¥ëŸ¬ë‹ ê¸°ì´ˆì™€ ì‹ ê²½ë§")
print("=" * 60)

# ì •ì±… íš¨ê³¼ ë¶„ë¥˜ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ MLP
def create_mlp_model(input_dim, hidden_units=[64, 32], output_dim=1):
    """
    ì •ì±… ì˜í–¥ ì˜ˆì¸¡ì„ ìœ„í•œ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  ìƒì„±

    ì¸ì:
        input_dim: ì…ë ¥ íŠ¹ì§•ì˜ ê°œìˆ˜
        hidden_units: ê° ì€ë‹‰ì¸µì˜ ë‰´ëŸ° ìˆ˜ ë¦¬ìŠ¤íŠ¸
        output_dim: ì¶œë ¥ í´ë˜ìŠ¤/ê°’ì˜ ê°œìˆ˜
    """
    model = keras.Sequential()
    model.add(keras.layers.Input(shape=(input_dim,)))
    
    # ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ê°€ì§„ ì€ë‹‰ì¸µ ì¶”ê°€
    for units in hidden_units:
        model.add(keras.layers.Dense(units, activation='relu'))
        model.add(keras.layers.BatchNormalization())
        model.add(keras.layers.Dropout(0.2))
    
    # ì¶œë ¥ì¸µ
    model.add(keras.layers.Dense(output_dim, activation='sigmoid'))
    
    return model

# ì •ì±… ì˜ˆì¸¡ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜
def mse_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

def mape_loss(y_true, y_pred):
    epsilon = 1e-7  # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
    percentage_error = tf.abs((y_true - y_pred) / (y_true + epsilon))
    return tf.reduce_mean(percentage_error) * 100

def policy_aware_loss(y_true, y_pred, policy_phase):
    """
    ì •ì±… ë‹¨ê³„ì— ë”°ë¼ ì˜¤ë¥˜ì— ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜
    """
    base_loss = mse_loss(y_true, y_pred)
    
    # ì •ì±… ì‹œí–‰ ê¸°ê°„ ë™ì•ˆ ë” ë†’ì€ í˜ë„í‹° ë¶€ì—¬
    policy_weight = tf.where(policy_phase > 0, 2.0, 1.0)
    weighted_loss = base_loss * policy_weight
    
    return tf.reduce_mean(weighted_loss)

# Lion ì˜µí‹°ë§ˆì´ì € êµ¬í˜„
class Lion(keras.optimizers.Optimizer):
    def __init__(self, learning_rate=1e-4, beta_1=0.9, beta_2=0.99, 
                 weight_decay=0.0, name="Lion", **kwargs):
        super().__init__(name=name, **kwargs)
        self._learning_rate = learning_rate
        self._beta_1 = beta_1
        self._beta_2 = beta_2
        self._weight_decay = weight_decay
        
    def build(self, var_list):
        super().build(var_list)
        self._momentums = []
        for var in var_list:
            self._momentums.append(
                self.add_variable_from_reference(var, "momentum")
            )

# íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜
def create_temporal_features(df):
    """ì „ë ¥ ìˆ˜ìš”ë¥¼ ìœ„í•œ ìˆ˜ì‘ì—… íŠ¹ì§• ìƒì„±"""
    df['hour'] = df['timestamp'].dt.hour
    df['dayofweek'] = df['timestamp'].dt.dayofweek
    df['month'] = df['timestamp'].dt.month
    df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
    
    # í•œêµ­ ê³µíœ´ì¼ (ê°„ì†Œí™”ëœ ëª©ë¡)
    korean_holidays = pd.to_datetime([
        '2024-01-01', '2024-02-09', '2024-02-10', '2024-02-11',
        '2024-03-01', '2024-05-05', '2024-05-15', '2024-06-06',
        '2024-08-15', '2024-09-16', '2024-09-17', '2024-09-18',
        '2024-10-03', '2024-10-09', '2024-12-25'
    ])
    df['is_holiday'] = df['timestamp'].isin(korean_holidays).astype(int)
    
    # ìˆœí™˜ ì¸ì½”ë”©
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    
    return df

# íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ ì˜¤í† ì¸ì½”ë”
class AutoEncoder(keras.Model):
    """ë”¥ ì˜¤í† ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œ ìë™ íŠ¹ì§• ì¶”ì¶œ"""
    def __init__(self, input_dim, encoding_dim=32):
        super(AutoEncoder, self).__init__()
        self.encoder = keras.Sequential([
            keras.layers.Dense(128, activation='relu'),
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dense(encoding_dim, activation='relu')
        ])
        self.decoder = keras.Sequential([
            keras.layers.Dense(64, activation='relu'),
            keras.layers.Dense(128, activation='relu'),
            keras.layers.Dense(input_dim, activation='sigmoid')
        ])
    
    def call(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

def preprocess_policy_data(df):
    """
    íŠ¹ë³„í•œ ê³ ë ¤ì‚¬í•­ì„ í¬í•¨í•œ ì •ì±… ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
    """
    # ì „ì§„ ì±„ìš°ê¸°ë¡œ ê²°ì¸¡ê°’ ì²˜ë¦¬ (ì •ì±… ì—°ì†ì„± ê°€ì •)
    df = df.fillna(method='ffill')
    
    # ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ì„ ì‚¬ìš©í•œ ì •ê·œí™” (ì´ìƒì¹˜ì— ê°•í•¨)
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
    
    # ì •ì±… ê°œì… ì§€í‘œ ë¶„ë¦¬
    policy_cols = ['policy_intervention', 'policy_phase']
    feature_cols = [col for col in df.columns if col not in policy_cols and col != 'timestamp']
    
    # ì—°ì† íŠ¹ì§• ìŠ¤ì¼€ì¼ë§
    if feature_cols:
        df[feature_cols] = scaler.fit_transform(df[feature_cols])
    
    # ì§€ì—°ëœ ì •ì±… íš¨ê³¼ë¥¼ ìœ„í•œ ì‹œì°¨ íŠ¹ì§• ìƒì„±
    if 'demand_mw' in df.columns:
        for lag in [1, 7, 30]:  # 1ì¼, 1ì£¼ì¼, 1ê°œì›” ì‹œì°¨
            df[f'demand_lag_{lag}'] = df['demand_mw'].shift(lag)
        
        # ì´ë™ í†µê³„ ìƒì„±
        for window in [24, 168]:  # ì¼ë³„ ë° ì£¼ë³„ ìœˆë„ìš°
            df[f'demand_ma_{window}'] = df['demand_mw'].rolling(window).mean()
            df[f'demand_std_{window}'] = df['demand_mw'].rolling(window).std()
    
    return df

print("\nâœ… Deep learning components initialized")

# =====================================================
# Section 3.2: ì‹œê³„ì—´ ë°ì´í„°ì™€ ì •ì±… ë¶„ì„
# =====================================================

print("\n" + "=" * 60)
print("Section 3.2: ì‹œê³„ì—´ ë°ì´í„°ì™€ ì •ì±… ë¶„ì„")
print("=" * 60)

def detect_intervention_points(ts_data, threshold=3):
    """
    í†µê³„ì  ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì •ì±… ê°œì… ì‹œì  ê°ì§€
    """
    # ì´ë™ í†µê³„ ê³„ì‚°
    window = 30
    rolling_mean = ts_data.rolling(window).mean()
    rolling_std = ts_data.rolling(window).std()
    
    # ì´ìƒì¹˜ ê°ì§€ë¥¼ ìœ„í•œ Z-ì ìˆ˜
    z_scores = np.abs(stats.zscore(ts_data.dropna()))
    
    # CUSUMì„ ì‚¬ìš©í•œ êµ¬ì¡°ì  ë³€í™” ê°ì§€
    def cusum(data):
        mean = np.mean(data)
        cumsum = np.cumsum(data - mean)
        return cumsum
    
    cusum_values = cusum(ts_data.dropna())
    
    # ê°œì… ì‹œì  ì‹ë³„
    interventions = []
    for i in range(len(z_scores)):
        if z_scores[i] > threshold:
            interventions.append({
                'index': i,
                'value': ts_data.iloc[i] if hasattr(ts_data, 'iloc') else ts_data[i],
                'z_score': z_scores[i],
                'type': 'anomaly'
            })
    
    return interventions

# ì •ìƒì„± í…ŒìŠ¤íŠ¸
from statsmodels.tsa.stattools import adfuller

def test_stationarity(timeseries, significance_level=0.05):
    """
    ADF í…ŒìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ì˜ ì •ìƒì„± ê²€ì‚¬
    """
    result = adfuller(timeseries.dropna())
    
    print(f'ADF Statistic: {result[0]:.4f}')
    print(f'p-value: {result[1]:.4f}')
    print(f'Critical Values:')
    for key, value in result[4].items():
        print(f'   {key}: {value:.3f}')
    
    if result[1] <= significance_level:
        print("âœ… Series is stationary")
        return True
    else:
        print("âŒ Series is non-stationary, differencing needed")
        return False

def make_stationary(ts_data):
    """
    ë¹„ì •ìƒ ì‹œê³„ì—´ì„ ì •ìƒ ì‹œê³„ì—´ë¡œ ë³€í™˜
    """
    # 1ì°¨ ì°¨ë¶„
    diff_1 = ts_data.diff().dropna()
    
    if test_stationarity(diff_1):
        return diff_1, 1
    
    # í•„ìš”ì‹œ 2ì°¨ ì°¨ë¶„
    diff_2 = diff_1.diff().dropna()
    if test_stationarity(diff_2):
        return diff_2, 2
    
    # ë¡œê·¸ ë³€í™˜ + ì°¨ë¶„
    log_diff = np.log(ts_data[ts_data > 0]).diff().dropna()
    return log_diff, 'log_diff'

# ì™¸ë¶€ ë³€ìˆ˜ë¥¼ ìœ„í•œ ë‹¤ë³€ëŸ‰ LSTM
class MultivariateLSTM(keras.Model):
    """
    ì •ì±… ì˜ˆì¸¡ì„ ìœ„í•œ ì™¸ë¶€ ë³€ìˆ˜ê°€ í¬í•¨ëœ LSTM ëª¨ë¸
    """
    def __init__(self, n_features, n_external, lstm_units=50):
        super(MultivariateLSTM, self).__init__()
        
        # ì‹œê³„ì—´ì„ ìœ„í•œ LSTM
        self.lstm_1 = keras.layers.LSTM(lstm_units, return_sequences=True)
        self.lstm_2 = keras.layers.LSTM(lstm_units // 2)
        
        # ì™¸ë¶€ ë³€ìˆ˜ë¥¼ ìœ„í•œ ë°€ì§‘ì¸µ
        self.external_dense = keras.layers.Dense(32, activation='relu')
        
        # ê²°í•©ì¸µ
        self.combine = keras.layers.Concatenate()
        self.output_dense = keras.layers.Dense(1)
    
    def call(self, inputs):
        ts_input, external_input = inputs
        
        # ì‹œê³„ì—´ ì²˜ë¦¬
        lstm_out = self.lstm_1(ts_input)
        lstm_out = self.lstm_2(lstm_out)
        
        # ì™¸ë¶€ ë³€ìˆ˜ ì²˜ë¦¬
        external_out = self.external_dense(external_input)
        
        # ê²°í•© ë° ì¶œë ¥
        combined = self.combine([lstm_out, external_out])
        output = self.output_dense(combined)
        
        return output

def generate_counterfactual(model, data, intervention_start, intervention_end):
    """
    ì •ì±… ê°œì… ì—†ì´ ë°˜ì‚¬ì‹¤ì  ì˜ˆì¸¡ ìƒì„±
    """
    # ë°ì´í„° ë³µì‚¬ ë° ê°œì… ì œê±°
    counterfactual_data = data.copy()
    counterfactual_data.loc[intervention_start:intervention_end, 'policy_intervention'] = 0
    
    # ê°œì…ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ì˜ ì˜ˆì¸¡
    with_policy = model.predict(data)
    without_policy = model.predict(counterfactual_data)
    
    # ì •ì±… íš¨ê³¼ ê³„ì‚°
    policy_effect = with_policy - without_policy
    
    return {
        'with_policy': with_policy,
        'without_policy': without_policy,
        'policy_effect': policy_effect,
        'average_effect': np.mean(policy_effect[intervention_start:intervention_end])
    }

# ë™ì  ì²˜ë¦¬ íš¨ê³¼ ì¶”ì •
class DynamicTreatmentEffect(keras.Model):
    """
    ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ì²˜ë¦¬ íš¨ê³¼ ì¶”ì •
    """
    def __init__(self, hidden_dim=64):
        super(DynamicTreatmentEffect, self).__init__()
        
        # ê³µìœ  í‘œí˜„ì¸µ
        self.shared = keras.Sequential([
            keras.layers.Dense(hidden_dim, activation='relu'),
            keras.layers.Dense(hidden_dim // 2, activation='relu')
        ])
        
        # ì²˜ë¦¬ë³„ í—¤ë“œ
        self.control_head = keras.layers.Dense(1)
        self.treatment_head = keras.layers.Dense(1)
    
    def call(self, inputs, treatment):
        # ê³µìœ  í‘œí˜„
        shared_rep = self.shared(inputs)
        
        # ì²˜ë¦¬ë³„ ì˜ˆì¸¡
        if treatment == 0:
            return self.control_head(shared_rep)
        else:
            return self.treatment_head(shared_rep)
    
    def estimate_effect(self, inputs):
        """ê°œë³„ ì²˜ë¦¬ íš¨ê³¼ ì¶”ì •"""
        y0 = self.call(inputs, treatment=0)
        y1 = self.call(inputs, treatment=1)
        return y1 - y0

print("\nâœ… Time series analysis tools initialized")

# =====================================================
# Section 3.3: RNNì˜ êµ¬ì¡°ì™€ í•œê³„
# =====================================================

print("\n" + "=" * 60)
print("Section 3.3: RNNì˜ êµ¬ì¡°ì™€ í•œê³„")
print("=" * 60)

class SimpleRNN(keras.Model):
    """
    ë©”ì»¤ë‹ˆì¦˜ ì´í•´ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ RNN êµ¬í˜„
    """
    def __init__(self, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        
        # ê°€ì¤‘ì¹˜
        self.W_xh = keras.layers.Dense(hidden_size, use_bias=False)
        self.W_hh = keras.layers.Dense(hidden_size, use_bias=False)
        self.W_hy = keras.layers.Dense(output_size, use_bias=False)
        
        # í¸í–¥
        self.b_h = self.add_weight(shape=(hidden_size,), initializer='zeros')
        self.b_y = self.add_weight(shape=(output_size,), initializer='zeros')
    
    def call(self, inputs, initial_hidden=None):
        batch_size, seq_len, input_size = inputs.shape
        
        if initial_hidden is None:
            hidden = tf.zeros((batch_size, self.hidden_size))
        else:
            hidden = initial_hidden
        
        outputs = []
        
        for t in range(seq_len):
            x_t = inputs[:, t, :]
            
            # ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸
            hidden = tf.nn.tanh(
                self.W_xh(x_t) + self.W_hh(hidden) + self.b_h
            )
            
            # ì¶œë ¥ ê³„ì‚°
            output = self.W_hy(hidden) + self.b_y
            outputs.append(output)
        
        return tf.stack(outputs, axis=1), hidden

# ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ì„ ìœ„í•œ ì‹¤ìš© RNN
def build_rnn_model(seq_length, n_features, n_outputs=24):
    """
    24ì‹œê°„ ì˜ˆì¸¡ì„ ìœ„í•œ RNN ëª¨ë¸ êµ¬ì¶•
    """
    model = keras.Sequential([
        keras.layers.SimpleRNN(64, return_sequences=True, 
                               input_shape=(seq_length, n_features)),
        keras.layers.Dropout(0.2),
        keras.layers.SimpleRNN(32),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(n_outputs)
    ])
    
    model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae', 'mape']
    )
    
    return model

def visualize_gradient_flow(model, X, y):
    """
    RNN ì¸µì„ í†µí•œ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ì‹œê°í™”
    """
    with tf.GradientTape() as tape:
        predictions = model(X)
        loss = tf.reduce_mean(tf.square(y - predictions))
    
    gradients = tape.gradient(loss, model.trainable_variables)
    
    gradient_norms = []
    for grad, var in zip(gradients, model.trainable_variables):
        if grad is not None:
            norm = tf.norm(grad).numpy()
            gradient_norms.append({
                'layer': var.name,
                'gradient_norm': norm,
                'vanishing': norm < 1e-5,
                'exploding': norm > 1e3
            })
    
    return gradient_norms

print("\nâœ… RNN components initialized")

# =====================================================
# Section 3.4: LSTMê³¼ GRU
# =====================================================

print("\n" + "=" * 60)
print("Section 3.4: LSTMê³¼ GRU")
print("=" * 60)

# ì •ì±… ì¸ì‹ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
class PolicyAwareAttention(keras.Model):
    """
    ê°œë…ì  ì •ì±… ì¸ì‹ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
    """
    def __init__(self, hidden_size):
        super(PolicyAwareAttention, self).__init__()
        self.lstm = keras.layers.LSTM(hidden_size, return_sequences=True)
        self.attention = keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=hidden_size // 4
        )
        self.output_layer = keras.layers.Dense(1)
    
    def call(self, inputs, policy_indicators=None):
        # í‘œì¤€ LSTM ì¸ì½”ë”©
        lstm_out = self.lstm(inputs)
        
        # ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ ì ìš©
        attended = self.attention(lstm_out, lstm_out, lstm_out)
        
        # ì—¬ê¸°ì— ì •ì±… ë‹¨ê³„ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
        if policy_indicators is not None:
            # ê°„ë‹¨í•œ ë‹¨ê³„ ì¸ì‹ ê°€ì¤‘ì¹˜ ì˜ˆì œ
            phase_weights = tf.expand_dims(policy_indicators, -1)
            attended = attended * (1 + phase_weights * 0.1)
        
        output = self.output_layer(attended)
        return output

def compare_lstm_gru_efficiency():
    """
    LSTMê³¼ GRUì˜ ê³„ì‚° íš¨ìœ¨ì„± ë¹„êµ
    """
    import time
    
    seq_length = 168  # 1ì£¼ì¼ ì‹œê°„ë‹¹ ë°ì´í„°
    n_features = 10
    batch_size = 32
    
    # ë”ë¯¸ ë°ì´í„° ìƒì„±
    X = tf.random.normal((batch_size, seq_length, n_features))
    
    # LSTM ëª¨ë¸
    lstm_model = keras.Sequential([
        keras.layers.LSTM(64, return_sequences=True),
        keras.layers.LSTM(32),
        keras.layers.Dense(24)
    ])
    
    # GRU ëª¨ë¸
    gru_model = keras.Sequential([
        keras.layers.GRU(64, return_sequences=True),
        keras.layers.GRU(32),
        keras.layers.Dense(24)
    ])
    
    # LSTM ì‹œê°„ ì¸¡ì •
    start = time.time()
    for _ in range(100):
        _ = lstm_model(X)
    lstm_time = time.time() - start
    
    # GRU ì‹œê°„ ì¸¡ì •
    start = time.time()
    for _ in range(100):
        _ = gru_model(X)
    gru_time = time.time() - start
    
    print(f"LSTM time: {lstm_time:.2f}s")
    print(f"GRU time: {gru_time:.2f}s")
    print(f"GRU is {lstm_time/gru_time:.1f}x faster")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
    lstm_params = lstm_model.count_params()
    gru_params = gru_model.count_params()
    
    print(f"LSTM parameters: {lstm_params:,}")
    print(f"GRU parameters: {gru_params:,}")
    print(f"GRU has {(1-gru_params/lstm_params)*100:.1f}% fewer parameters")

# ê°„ì†Œí™”ëœ Mamba ìƒíƒœ ê³µê°„ ëª¨ë¸
class MambaBlock(keras.Model):
    """
    ê°„ì†Œí™”ëœ Mamba ìƒíƒœ ê³µê°„ ëª¨ë¸ ë¸”ë¡
    """
    def __init__(self, d_model, d_state=16, expand=2):
        super(MambaBlock, self).__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.expand = expand
        d_inner = int(self.expand * self.d_model)
        
        # íˆ¬ì˜ì¸µ
        self.in_proj = keras.layers.Dense(d_inner * 2, use_bias=False)
        self.out_proj = keras.layers.Dense(d_model, use_bias=False)
        
        # SSM íŒŒë¼ë¯¸í„°
        self.A = self.add_weight(
            shape=(d_inner, d_state),
            initializer='glorot_uniform',
            trainable=False
        )
        self.B = keras.layers.Dense(d_state, use_bias=False)
        self.C = keras.layers.Dense(d_inner, use_bias=False)
        self.D = self.add_weight(shape=(d_inner,), initializer='ones')
        
        # ì´ì‚°í™” íŒŒë¼ë¯¸í„°
        self.delta = keras.layers.Dense(d_inner, use_bias=False)
    
    def selective_scan(self, x, delta, A, B, C, D):
        """
        í•˜ë“œì›¨ì–´ ì¸ì‹ êµ¬í˜„ì„ ê°€ì§„ ì„ íƒì  ìŠ¤ìº” ì•Œê³ ë¦¬ì¦˜
        """
        batch, length, d_inner = x.shape
        
        # ì—°ì† íŒŒë¼ë¯¸í„° ì´ì‚°í™”
        deltaA = tf.exp(tf.einsum('bld,dn->bldn', delta, A))
        deltaB = tf.einsum('bld,bln->bldn', delta, B)
        
        # ì„ íƒì  ìŠ¤ìº”
        states = []
        state = tf.zeros((batch, self.d_state, d_inner))
        
        for i in range(length):
            state = deltaA[:, i] * state + deltaB[:, i] * tf.expand_dims(x[:, i], 1)
            y = tf.einsum('bdn,bn->bd', state, C[:, i])
            states.append(y)
        
        return tf.stack(states, axis=1)
    
    def call(self, x):
        batch, length, _ = x.shape
        
        # ì…ë ¥ íˆ¬ì˜
        x_proj = self.in_proj(x)
        x, z = tf.split(x_proj, 2, axis=-1)
        
        # SSM ë¶„ê¸°
        delta = self.delta(x)
        B = self.B(x)
        C = self.C(x)
        
        # ì„ íƒì  ìŠ¤ìº” ì ìš©
        y = self.selective_scan(x, delta, self.A, B, C, self.D)
        
        # ê²Œì´íŠ¸ ì—°ê²°
        y = y * tf.nn.silu(z)
        
        # ì¶œë ¥ íˆ¬ì˜
        output = self.out_proj(y)
        
        return output

# ê°œë…ì  í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸
class ConceptualHybridModel(keras.Model):
    """
    ê°œë…ì  í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ (ê²½í—˜ì ìœ¼ë¡œ ê²€ì¦ë˜ì§€ ì•ŠìŒ)
    ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ê³¼ ìƒíƒœ ê³µê°„ ëª¨ë¸ì˜ ê²°í•©
    """
    def __init__(self, d_model=256, n_heads=8, n_layers=6):
        super(ConceptualHybridModel, self).__init__()
        
        # ì´ê²ƒì€ ê°œë…ì  ì˜ˆì œë¡œ, ì¶œíŒëœ ì—°êµ¬ì— ê¸°ë°˜í•˜ì§€ ì•ŠìŒ
        self.attention_layer = keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads
        )
        self.feedforward = keras.layers.Dense(d_model)
        
    def call(self, x):
        # ê°„ë‹¨í•œ ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
        attended = self.attention_layer(x, x, x)
        output = self.feedforward(attended)
        return output

print("\nâœ… LSTM/GRU/Mamba components initialized")
print("\nRunning efficiency comparison...")
compare_lstm_gru_efficiency()

# =====================================================
# Section 3.5: ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤ìŠµ
# =====================================================

print("\n" + "=" * 60)
print("Section 3.5: ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹¤ìŠµ")
print("=" * 60)

# ë°ì´í„° ìƒì„± ë° ì €ì¥
def generate_and_save_data(output_dir='data'):
    """
    í•©ì„± ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥
    """
    np.random.seed(42)

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)

    # ë‚ ì§œ ë²”ìœ„ ìƒì„± (2024ë…„ ì „ì²´, ì‹œê°„ë‹¹ ë°ì´í„°)
    dates = pd.date_range(start='2024-01-01', end='2024-12-31 23:00:00', freq='h')

    # ê¸°ë³¸ ìˆ˜ìš” íŒ¨í„´ (MW)
    base_demand = 65000
    hourly_pattern = np.array([0.7, 0.65, 0.6, 0.58, 0.57, 0.58,
                               0.65, 0.75, 0.85, 0.9, 0.92, 0.94,
                               0.93, 0.92, 0.93, 0.94, 0.95, 0.93,
                               0.9, 0.85, 0.8, 0.75, 0.72, 0.71])

    # í•©ì„± ìˆ˜ìš” ë°ì´í„° ìƒì„±
    demand_data = []
    for date in dates:
        hour_factor = hourly_pattern[date.hour]
        seasonal_factor = 1.1 if date.month in [7, 8, 12, 1, 2] else 1.0
        weekend_factor = 0.85 if date.weekday() >= 5 else 1.0
        noise = np.random.normal(0, 0.02)
        demand = base_demand * hour_factor * seasonal_factor * weekend_factor * (1 + noise)
        demand_data.append(demand)

    # ìˆ˜ìš” ë°ì´í„°í”„ë ˆì„ ìƒì„±
    demand_df = pd.DataFrame({
        'timestamp': dates,
        'demand_mw': demand_data,
        'temperature': 15 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (365*24)) +
                      np.random.normal(0, 2, len(dates)),
        'solar_generation_mw': np.maximum(0, 5000 * np.sin(np.pi * dates.hour / 24) *
                                         (1 - 0.3 * np.random.random(len(dates)))),
        'wind_generation_mw': 2000 + 1000 * np.random.random(len(dates)),
        'is_holiday': np.random.choice([0, 1], len(dates), p=[0.95, 0.05])
    })

    # ì •ì±… ë°ì´í„°í”„ë ˆì„ ìƒì„±
    policy_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='D')
    policy_df = pd.DataFrame({
        'date': policy_dates,
        'policy_phase': np.random.choice([0, 1, 2, 3, 4], len(policy_dates),
                                        p=[0.5, 0.2, 0.15, 0.1, 0.05]),
        'policy_intervention': np.random.choice([0, 1], len(policy_dates), p=[0.8, 0.2]),
        'renewable_target': np.linspace(20, 35, len(policy_dates)),
        'carbon_price': 50000 + 10000 * np.random.random(len(policy_dates)),
        'rec_price': 80000 + 20000 * np.random.random(len(policy_dates))
    })

    # ì‹œì¥ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ì›”ë³„)
    market_dates = pd.date_range(start='2024-01-01', end='2024-12-31', freq='M')
    market_df = pd.DataFrame({
        'date': market_dates,
        'smp_avg': 120000 + 30000 * np.random.random(len(market_dates)),
        'demand_avg': 65000 + 5000 * np.random.random(len(market_dates)),
        'renewable_ratio': np.linspace(15, 25, len(market_dates)) + np.random.normal(0, 2, len(market_dates))
    })

    # CSV íŒŒì¼ë¡œ ì €ì¥
    demand_df.to_csv(os.path.join(output_dir, 'energy_demand.csv'), index=False)
    policy_df.to_csv(os.path.join(output_dir, 'renewable_policy.csv'), index=False)
    market_df.to_csv(os.path.join(output_dir, 'electricity_market.csv'), index=False)

    print(f"âœ… ë°ì´í„°ê°€ ìƒì„±ë˜ì–´ '{output_dir}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"   - energy_demand.csv: {len(demand_df)} ì‹œê°„ë³„ ë ˆì½”ë“œ")
    print(f"   - renewable_policy.csv: {len(policy_df)} ì¼ë³„ ë ˆì½”ë“œ")
    print(f"   - electricity_market.csv: {len(market_df)} ì›”ë³„ ë ˆì½”ë“œ")

    return demand_df, policy_df, market_df

# ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
def load_and_prepare_data(data_dir='data', generate_if_missing=True):
    """
    ì €ì¥ëœ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ëª¨ë¸ë§ì„ ìœ„í•´ ì¤€ë¹„
    """
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    demand_file = os.path.join(data_dir, 'energy_demand.csv')
    policy_file = os.path.join(data_dir, 'renewable_policy.csv')
    market_file = os.path.join(data_dir, 'electricity_market.csv')

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not all(os.path.exists(f) for f in [demand_file, policy_file, market_file]):
        if generate_if_missing:
            print("âš ï¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            demand_df, policy_df, market_df = generate_and_save_data(data_dir)
        else:
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ {data_dir} í´ë”ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“‚ '{data_dir}' í´ë”ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        demand_df = pd.read_csv(demand_file)
        demand_df['timestamp'] = pd.to_datetime(demand_df['timestamp'])

        policy_df = pd.read_csv(policy_file)
        policy_df['date'] = pd.to_datetime(policy_df['date'])

        market_df = pd.read_csv(market_file)
        market_df['date'] = pd.to_datetime(market_df['date'])

        print(f"âœ… ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤:")
        print(f"   - ì—ë„ˆì§€ ìˆ˜ìš”: {len(demand_df)} ë ˆì½”ë“œ")
        print(f"   - ì •ì±… ë°ì´í„°: {len(policy_df)} ë ˆì½”ë“œ")
        print(f"   - ì‹œì¥ ë°ì´í„°: {len(market_df)} ë ˆì½”ë“œ")

    # ì‹œê°„ì  íŠ¹ì§• ì¶”ê°€
    demand_df = create_temporal_features(demand_df)

    # ì •ì±… ì •ë³´ ë³‘í•©
    demand_df['date'] = demand_df['timestamp'].dt.date
    policy_df['date_only'] = policy_df['date'].dt.date

    demand_df = demand_df.merge(
        policy_df[['date_only', 'policy_phase', 'policy_intervention']],
        left_on='date',
        right_on='date_only',
        how='left'
    )
    demand_df.drop(['date', 'date_only'], axis=1, inplace=True)

    # ëˆ„ë½ëœ ì •ì±… ê°’ ì±„ìš°ê¸°
    demand_df['policy_phase'] = demand_df['policy_phase'].fillna(0).astype(int)
    demand_df['policy_intervention'] = demand_df['policy_intervention'].fillna(0).astype(int)

    # ì‹œì°¨ íŠ¹ì§• ì¶”ê°€
    for lag in [24, 48, 168]:  # 1ì¼, 2ì¼, 1ì£¼ì¼
        demand_df[f'demand_lag_{lag}'] = demand_df['demand_mw'].shift(lag)

    # ì´ë™ í†µê³„ ì¶”ê°€
    for window in [24, 168]:  # ì¼ë³„ ë° ì£¼ë³„ ìœˆë„ìš°
        demand_df[f'demand_ma_{window}'] = demand_df['demand_mw'].rolling(window).mean()
        demand_df[f'demand_std_{window}'] = demand_df['demand_mw'].rolling(window).std()

    # NaN ê°’ ì œê±°
    demand_df = demand_df.dropna()

    print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {demand_df.shape[0]} ìƒ˜í”Œ, {demand_df.shape[1]} íŠ¹ì§•")

    return demand_df, policy_df


# LSTM í›ˆë ¨ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„±
def create_sequences(data, target_col, seq_length=168, pred_length=24):
    """
    ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ìƒì„±

    ì¸ì:
        data: íŠ¹ì§•ì„ ê°€ì§„ DataFrame
        target_col: ëŒ€ìƒ ì»´ëŸ¼ ì´ë¦„
        seq_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (168 = 1ì£¼ì¼)
        pred_length: ì˜ˆì¸¡ ê¸¸ì´ (24 = 1ì¼ ì•)
    """
    # timestamp ì»´ëŸ¼ì´ ìˆìœ¼ë©´ ì œê±°, ì—†ìœ¼ë©´ target_colë§Œ ì œê±°
    cols_to_drop = [col for col in ['timestamp', target_col] if col in data.columns]
    features = data.drop(columns=cols_to_drop).values
    target = data[target_col].values
    
    X, y = [], []
    
    for i in range(len(data) - seq_length - pred_length + 1):
        X.append(features[i:i+seq_length])
        y.append(target[i+seq_length:i+seq_length+pred_length])
    
    return np.array(X), np.array(y)

# ì •ì±… ì¸ì‹ LSTM ëª¨ë¸
class PolicyAwareLSTM(keras.Model):
    """
    ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ì„ ìœ„í•œ ì •ì±… ì¸ì‹ êµ¬ì„± ìš”ì†Œê°€ í¬í•¨ëœ LSTM ëª¨ë¸
    """
    def __init__(self, n_features, lstm_units=[64, 32], pred_length=24):
        super(PolicyAwareLSTM, self).__init__()
        
        # LSTM ì¸µ
        self.lstm_layers = []
        for i, units in enumerate(lstm_units):
            return_seq = (i < len(lstm_units) - 1)
            self.lstm_layers.append(
                keras.layers.LSTM(units, return_sequences=return_seq, 
                                 dropout=0.2, recurrent_dropout=0.2)
            )
        
        # ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = keras.layers.MultiHeadAttention(
            num_heads=4, key_dim=lstm_units[-1] // 4
        )
        
        # ë°€ì§‘ì¸µ
        self.dense1 = keras.layers.Dense(64, activation='relu')
        self.dropout = keras.layers.Dropout(0.3)
        self.dense2 = keras.layers.Dense(pred_length)
    
    def call(self, inputs, training=None):
        x = inputs
        
        # LSTM ì¸µ í†µê³¼
        for lstm in self.lstm_layers:
            x = lstm(x, training=training)
        
        # ìê¸° ì£¼ì˜
        attended = self.attention(
            tf.expand_dims(x, 1),
            tf.expand_dims(x, 1),
            tf.expand_dims(x, 1)
        )
        x = tf.squeeze(attended, 1)
        
        # ë°€ì§‘ì¸µ
        x = self.dense1(x)
        x = self.dropout(x, training=training)
        output = self.dense2(x)
        
        return output

# ëª¨ë¸ êµ¬ì¶• ë° ì»´íŒŒì¼
def build_lstm_model(input_shape, pred_length=24):
    """
    ìˆ˜ìš” ì˜ˆì¸¡ì„ ìœ„í•œ LSTM ëª¨ë¸ êµ¬ì¶• ë° ì»´íŒŒì¼
    """
    model = PolicyAwareLSTM(
        n_features=input_shape[-1],
        lstm_units=[128, 64, 32],
        pred_length=pred_length
    )
    
    # ì»¤ìŠ¤í…€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„
    lr_schedule = keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-3,
        decay_steps=1000,
        decay_rate=0.9
    )
    
    # ì»¤ìŠ¤í…€ ì†ì‹¤ë¡œ ì»´íŒŒì¼
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),
        loss='mse',
        metrics=['mae', keras.metrics.MeanAbsolutePercentageError()]
    )
    
    return model

# ì •ì±… ë‹¨ê³„ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ì½œë°±
class PolicyPhaseCallback(keras.callbacks.Callback):
    """
    ì •ì±… ë‹¨ê³„ì— ê±¸ì³ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±
    """
    def __init__(self, validation_data, policy_phases):
        super().__init__()
        self.validation_data = validation_data
        self.policy_phases = policy_phases
        self.phase_metrics = {phase: [] for phase in range(5)}
    
    def on_epoch_end(self, epoch, logs=None):
        X_val, y_val = self.validation_data
        predictions = self.model.predict(X_val, verbose=0)
        
        for phase in range(5):
            phase_mask = (self.policy_phases == phase)
            if np.any(phase_mask):
                phase_mae = np.mean(np.abs(y_val[phase_mask] - predictions[phase_mask]))
                self.phase_metrics[phase].append(phase_mae)
                
        if epoch % 10 == 0:
            print(f"\nPolicy Phase Performance (MAE):")
            for phase, metrics in self.phase_metrics.items():
                if metrics:
                    print(f"  Phase {phase}: {metrics[-1]:.2f}")

# ì‹œê°í™” í•¨ìˆ˜
def visualize_training_history(history):
    """
    í›ˆë ¨ ì´ë ¥ ì‹œê°í™”
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # ì†ì‹¤ ê·¸ë˜í”„
    axes[0].plot(history.history['loss'], label='Training Loss')
    axes[0].plot(history.history['val_loss'], label='Validation Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss (MSE)')
    axes[0].set_title('Model Loss During Training')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # MAE ê·¸ë˜í”„
    axes[1].plot(history.history['mae'], label='Training MAE')
    axes[1].plot(history.history['val_mae'], label='Validation MAE')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('MAE')
    axes[1].set_title('Mean Absolute Error During Training')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('output/chapter3_training_history.png', dpi=150)
    plt.show()

def visualize_predictions(model, X_test, y_test, n_samples=3):
    """
    ëª¨ë¸ ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ ì‹œê°í™”
    """
    predictions = model.predict(X_test)
    
    fig, axes = plt.subplots(n_samples, 1, figsize=(15, 3*n_samples))
    if n_samples == 1:
        axes = [axes]
    
    for i in range(n_samples):
        idx = np.random.randint(0, len(X_test))
        
        axes[i].plot(y_test[idx], label='Actual', linewidth=2)
        axes[i].plot(predictions[idx], label='Predicted', linewidth=2, linestyle='--')
        axes[i].set_xlabel('Hours Ahead')
        axes[i].set_ylabel('Electricity Demand (Normalized)')
        axes[i].set_title(f'24-Hour Ahead Forecast - Sample {i+1}')
        axes[i].legend()
        axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('output/chapter3_predictions.png', dpi=150)
    plt.show()

def analyze_policy_impact(model, demand_df):
    """
    ì˜ˆì¸¡ì— ëŒ€í•œ ì •ì±… ê°œì…ì˜ ì˜í–¥ ë¶„ì„
    """
    # ì •ì±… ê°œì… ê¸°ê°„ ì‹ë³„
    policy_periods = demand_df.groupby('policy_phase')['demand_mw'].agg(['mean', 'std'])
    
    print("\nğŸ“Š Policy Phase Analysis:")
    print(policy_periods)
    
    # ì •ì±… ë‹¨ê³„ë³„ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚°
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # ì •ì±… ë‹¨ê³„ë³„ ìˆ˜ìš” ë¶„í¬
    demand_df.boxplot(column='demand_mw', by='policy_phase', ax=axes[0])
    axes[0].set_xlabel('Policy Phase')
    axes[0].set_ylabel('Demand (MW)')
    axes[0].set_title('Demand Distribution by Policy Phase')
    plt.sca(axes[0])
    plt.xticks(rotation=0)
    
    # ì‹œê°„ì— ë”°ë¥¸ ì¬ìƒ ì—ë„ˆì§€ ë°œì „
    axes[1].plot(demand_df.index[:1000], demand_df['solar_generation_mw'].iloc[:1000], 
                label='Solar', alpha=0.7)
    axes[1].plot(demand_df.index[:1000], demand_df['wind_generation_mw'].iloc[:1000], 
                label='Wind', alpha=0.7)
    axes[1].set_xlabel('Time')
    axes[1].set_ylabel('Generation (MW)')
    axes[1].set_title('Renewable Generation Pattern')
    axes[1].legend()
    
    plt.tight_layout()
    plt.savefig('output/chapter3_policy_analysis.png', dpi=150)
    plt.show()

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ ì‹œì‘: ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„")
    print("="*60)

    # ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
    DATA_DIR = 'data'
    OUTPUT_DIR = 'output'

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # ë°ì´í„° ë¡œë“œ (í•„ìš”ì‹œ ìë™ ìƒì„±)
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    demand_df, policy_df = load_and_prepare_data(data_dir=DATA_DIR, generate_if_missing=True)

    # ëª¨ë¸ë§ì„ ìœ„í•œ íŠ¹ì§• ì„ íƒ
    print("\nğŸ¯ íŠ¹ì§• ì„ íƒ ë° ì „ì²˜ë¦¬ ì¤‘...")

    # hour_sin, hour_cos íŠ¹ì§•ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ìƒì„±
    if 'hour_sin' not in demand_df.columns:
        demand_df['hour'] = demand_df['timestamp'].dt.hour
        demand_df['hour_sin'] = np.sin(2 * np.pi * demand_df['hour'] / 24)
        demand_df['hour_cos'] = np.cos(2 * np.pi * demand_df['hour'] / 24)

    if 'day_sin' not in demand_df.columns:
        demand_df['day'] = demand_df['timestamp'].dt.dayofyear
        demand_df['day_sin'] = np.sin(2 * np.pi * demand_df['day'] / 365)
        demand_df['day_cos'] = np.cos(2 * np.pi * demand_df['day'] / 365)

    # íŠ¹ì§• ì»¬ëŸ¼ ì„ íƒ
    feature_cols = [
        'demand_mw', 'solar_generation_mw', 'wind_generation_mw', 'temperature',
        'hour_sin', 'hour_cos', 'day_sin', 'day_cos',
        'is_holiday', 'demand_lag_24', 'demand_lag_48',
        'demand_lag_168', 'demand_ma_24', 'demand_std_24'
    ]

    # ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì§•ë§Œ í•„í„°ë§
    available_features = [col for col in feature_cols if col in demand_df.columns]

    # íŠ¹ì§• ì •ê·œí™”
    scaler = MinMaxScaler()
    demand_df[available_features] = scaler.fit_transform(demand_df[available_features])

    print(f"âœ… {len(available_features)}ê°œ íŠ¹ì§• ì‚¬ìš©: {', '.join(available_features[:5])}...")

    # ì‹œí€€ìŠ¤ ìƒì„±
    print("\nğŸ“¦ ì‹œí€€ìŠ¤ ìƒì„± ì¤‘...")

    # ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•´ demand_mwë¥¼ íŠ¹ì§•ì— í¬í•¨
    sequence_cols = available_features if 'demand_mw' in available_features else available_features + ['demand_mw']
    X, y = create_sequences(demand_df[sequence_cols], 'demand_mw')
    print(f"âœ… ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ: X shape {X.shape}, y shape {y.shape}")

    # ë°ì´í„° ë¶„í• : 70% í›ˆë ¨, 15% ê²€ì¦, 15% í…ŒìŠ¤íŠ¸
    print("\nğŸ”€ ë°ì´í„° ë¶„í•  ì¤‘...")
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, shuffle=False)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, shuffle=False)

    print(f"âœ… ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    print(f"   - í›ˆë ¨: {X_train.shape[0]} ìƒ˜í”Œ")
    print(f"   - ê²€ì¦: {X_val.shape[0]} ìƒ˜í”Œ")
    print(f"   - í…ŒìŠ¤íŠ¸: {X_test.shape[0]} ìƒ˜í”Œ")

    # ëª¨ë¸ ì´ˆê¸°í™”
    print("\nğŸ—ï¸ LSTM ëª¨ë¸ êµ¬ì¶• ì¤‘...")
    lstm_model = build_lstm_model(X_train.shape[1:])

    # í•œ ë²ˆ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ êµ¬ì¶•
    _ = lstm_model(X_train[:1])

    print("âœ… ëª¨ë¸ ì•„í‚¤í…ì²˜:")
    print(f"   ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum([tf.size(w).numpy() for w in lstm_model.trainable_weights]):,}")

    # í›ˆë ¨ ì„¤ì •
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True
        ),
        keras.callbacks.ModelCheckpoint(
            os.path.join(OUTPUT_DIR, 'lstm_best_model.keras'),
            monitor='val_loss',
            save_best_only=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6
        )
    ]

    # ëª¨ë¸ í›ˆë ¨ (ë°ëª¨ë¥¼ ìœ„í•´ ì—í¬í¬ ê°ì†Œ)
    print("\nğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
    print("-" * 40)

    history = lstm_model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=20,  # ë°ëª¨ë¥¼ ìœ„í•´ ê°ì†Œ
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )

    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ í‰ê°€
    print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    test_loss, test_mae, test_mape = lstm_model.evaluate(X_test, y_test)
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„±ëŠ¥:")
    print(f"   ì†ì‹¤ (MSE): {test_loss:.4f}")
    print(f"   MAE: {test_mae:.2f}")
    print(f"   MAPE: {test_mape:.2f}%")

    # ì‹œê°í™” ì‹¤í–‰
    print("\nğŸ“ˆ ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ì¤‘...")

    # í•™ìŠµ ì´ë ¥ ì‹œê°í™”
    visualize_training_history(history)

    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    visualize_predictions(lstm_model, X_test, y_test, n_samples=2)

    # ì •ì±… ì˜í–¥ ë¶„ì„
    analyze_policy_impact(lstm_model, demand_df)

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ† ë¶„ì„ ì™„ë£Œ!")
    print("="*60)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜:")
    print(f"   - ë°ì´í„°: {DATA_DIR}/")
    print(f"   - ëª¨ë¸: {OUTPUT_DIR}/lstm_best_model.keras")
    print(f"   - ê·¸ë˜í”„: {OUTPUT_DIR}/")
    print("\nâœ… ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì •ì±… ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("="*60)